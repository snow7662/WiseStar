"""
QualityEvaluator - é¢˜ç›®è´¨é‡è¯„ä¼°å™¨

å¯¹ç”Ÿæˆçš„é¢˜ç›®è¿›è¡Œå¤šç»´åº¦è´¨é‡è¯„ä¼°
"""

import re
from utils.llm import call_llm_stream


class QualityEvaluator:
    """é¢˜ç›®è´¨é‡è¯„ä¼°å™¨"""
    
    # è¯„åˆ†æ ‡å‡†
    ACCEPT_THRESHOLD = 7.0  # æ¥å—é¢˜ç›®çš„æœ€ä½ç»¼åˆè¯„åˆ†
    
    def __init__(self, accept_threshold: float = None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            accept_threshold: æ¥å—é¢˜ç›®çš„æœ€ä½åˆ†æ•°ï¼Œé»˜è®¤7.0
        """
        self.accept_threshold = accept_threshold or self.ACCEPT_THRESHOLD
        print(f"âœ… QualityEvaluatoråˆå§‹åŒ–æˆåŠŸ (æ¥å—é˜ˆå€¼: {self.accept_threshold})")
    
    def evaluate(self, problem: str, repi_result: dict, requirements: str = "") -> dict:
        """
        è¯„ä¼°é¢˜ç›®è´¨é‡
        
        Args:
            problem: ç”Ÿæˆçš„é¢˜ç›®æ–‡æœ¬
            repi_result: REPIéªŒè¯ç»“æœ
            requirements: å‡ºé¢˜è¦æ±‚
            
        Returns:
            dict: è¯„ä¼°ç»“æœï¼ŒåŒ…å«scores, decision, suggestionsç­‰
        """
        print(f"\nğŸ“Š [QualityEvaluator] å¼€å§‹è´¨é‡è¯„ä¼°...")
        
        # æ„å»ºè¯„ä¼°æç¤ºè¯
        prompt = self._build_evaluation_prompt(problem, repi_result, requirements)
        
        # è°ƒç”¨LLMè¿›è¡Œè¯„ä¼°
        print(f"ğŸ“Š [QualityEvaluator] è°ƒç”¨AIè¿›è¡Œè¯„ä¼°...")
        response = call_llm_stream(prompt)
        
        # è§£æè¯„åˆ†
        scores = self._parse_scores(response)
        
        # è§£æå†³ç­–å’Œå»ºè®®
        action_match = re.search(r'<action>(.*?)</action>', response, re.DOTALL)
        action = action_match.group(1).strip() if action_match else "refine"
        
        suggestions_match = re.search(r'<improvement_suggestions>(.*?)</improvement_suggestions>', 
                                     response, re.DOTALL)
        suggestions = suggestions_match.group(1).strip() if suggestions_match else ""
        
        strengths_match = re.search(r'<strengths>(.*?)</strengths>', response, re.DOTALL)
        strengths = strengths_match.group(1).strip() if strengths_match else ""
        
        weaknesses_match = re.search(r'<weaknesses>(.*?)</weaknesses>', response, re.DOTALL)
        weaknesses = weaknesses_match.group(1).strip() if weaknesses_match else ""
        
        # ç»¼åˆå†³ç­–
        overall_score = scores.get('overall_score', 0)
        decision = 'accept' if (action == 'accept' or overall_score >= self.accept_threshold) else 'refine'
        
        result = {
            'scores': scores,
            'decision': decision,
            'suggestions': suggestions,
            'strengths': strengths,
            'weaknesses': weaknesses,
            'raw_response': response
        }
        
        # æ‰“å°è¯„ä¼°ç»“æœ
        self._print_evaluation_result(scores, decision)
        
        return result
    
    def _build_evaluation_prompt(self, problem: str, repi_result: dict, requirements: str) -> str:
        """æ„å»ºè¯„ä¼°æç¤ºè¯"""
        
        stats = repi_result.get('statistics', {})
        
        prompt = f"""
è¯·åŸºäºREPIè§£é¢˜ç»“æœï¼Œè¯„ä¼°ä»¥ä¸‹çº¯AIç”Ÿæˆçš„æ•°å­¦é¢˜ç›®çš„è´¨é‡ï¼š

## ç”Ÿæˆçš„é¢˜ç›®
{problem}

## å‡ºé¢˜è¦æ±‚
{requirements if requirements else "æ— ç‰¹æ®Šè¦æ±‚"}

## REPIè§£é¢˜éªŒè¯ç»“æœ
- è§£é¢˜æˆåŠŸï¼š{repi_result.get('success', False)}
- è§£é¢˜ç­”æ¡ˆï¼š{repi_result.get('answer', 'æ— ç­”æ¡ˆ')[:200]}...
- æ€»è§£é¢˜æ­¥æ•°ï¼š{stats.get('total_steps', 0)}
- æ¨ç†æ­¥æ•°ï¼š{stats.get('reasoning_steps', 0)}
- è®¡ç®—æ­¥æ•°ï¼š{stats.get('calculation_steps', 0)}
- è®¡ç®—æˆåŠŸç‡ï¼š{stats.get('successful_calculations', 0)}/{stats.get('calculation_steps', 0)}

## è¯„ä¼°ç»´åº¦ï¼ˆçº¯AIæ¨¡å¼ï¼‰
1. **åŸåˆ›æ€§ä¸åˆ›æ–°æ€§**ï¼šé¢˜ç›®æ˜¯å¦å…·æœ‰åŸåˆ›æ€§ï¼Œé¿å…äº†å¸¸è§å¥—è·¯ (1-10åˆ†)
2. **å¯è§£æ€§**ï¼šREPIç³»ç»Ÿæ˜¯å¦èƒ½å¤ŸæˆåŠŸè§£å‡º (1-10åˆ†)
3. **å¤æ‚åº¦ä¸åŒºåˆ†åº¦**ï¼šä»¥é«˜è€ƒå‹è½´é¢˜ä¸ºåŸºå‡†ï¼Œè¯„ä¼°é¢˜ç›®éš¾åº¦å±‚æ¬¡ (1-10åˆ†)
4. **çŸ¥è¯†è¦†ç›–ä¸èåˆ**ï¼šæ˜¯å¦æœ‰æ•ˆèåˆå¤šä¸ªæ•°å­¦çŸ¥è¯†ç‚¹ (1-10åˆ†)
5. **æ•™å­¦ä»·å€¼**ï¼šæ˜¯å¦å…·æœ‰è‰¯å¥½çš„æ•™å­¦å’Œç»ƒä¹ ä»·å€¼ (1-10åˆ†)

### å¤æ‚åº¦è¯„åˆ†å‚è€ƒï¼š
- 1-2åˆ†ï¼šå½¢å¼å¤æ‚ä½†ç¼ºä¹æ€ç»´æ·±åº¦
- 3-4åˆ†ï¼šå‡†å‹è½´é¢˜æ°´å¹³ï¼Œå¸¸è§æ¨¡å‹åº”ç”¨
- 5-6åˆ†ï¼šæ ‡å‡†é«˜è€ƒå‹è½´é¢˜æ°´å¹³
- 7-8åˆ†ï¼šé¡¶å°–å‹è½´é¢˜ï¼Œéœ€è¦åˆ›é€ æ€§æ€ç»´
- 9-10åˆ†ï¼šç«èµ›çº§éš¾åº¦ï¼Œæ¢ç´¢AIèƒ½åŠ›è¾¹ç•Œ

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
<originality_score>1-10åˆ†</originality_score>
<solvability_score>1-10åˆ†</solvability_score>
<complexity_score>1-10åˆ†</complexity_score>
<knowledge_coverage_score>1-10åˆ†</knowledge_coverage_score>
<educational_value_score>1-10åˆ†</educational_value_score>
<overall_score>1-10åˆ†ï¼ˆç»¼åˆè¯„åˆ†ï¼‰</overall_score>
<strengths>é¢˜ç›®ä¼˜ç‚¹</strengths>
<weaknesses>é¢˜ç›®ç¼ºç‚¹</weaknesses>
<action>accept/refine</action>
<improvement_suggestions>æ”¹è¿›å»ºè®®ï¼ˆå¦‚æœéœ€è¦ï¼‰</improvement_suggestions>
"""
        return prompt
    
    def _parse_scores(self, response: str) -> dict:
        """è§£æè¯„åˆ†"""
        
        score_types = [
            'originality_score',
            'solvability_score', 
            'complexity_score',
            'knowledge_coverage_score',
            'educational_value_score',
            'overall_score'
        ]
        
        scores = {}
        
        for score_type in score_types:
            score_match = re.search(f'<{score_type}>(.*?)</{score_type}>', response, re.DOTALL)
            if score_match:
                score_str = score_match.group(1).strip()
                try:
                    # æå–æ•°å­—
                    number_match = re.search(r'(\d+(?:\.\d+)?)', score_str)
                    if number_match:
                        scores[score_type] = float(number_match.group(1))
                    else:
                        scores[score_type] = 5.0  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
                except Exception:
                    scores[score_type] = 5.0
            else:
                scores[score_type] = 5.0
        
        return scores
    
    def _print_evaluation_result(self, scores: dict, decision: str):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        
        print(f"\nğŸ“Š [QualityEvaluator] è¯„ä¼°ç»“æœ:")
        print(f"   - åŸåˆ›æ€§: {scores.get('originality_score', 0)}/10")
        print(f"   - å¯è§£æ€§: {scores.get('solvability_score', 0)}/10")
        print(f"   - å¤æ‚åº¦: {scores.get('complexity_score', 0)}/10")
        print(f"   - çŸ¥è¯†è¦†ç›–: {scores.get('knowledge_coverage_score', 0)}/10")
        print(f"   - æ•™å­¦ä»·å€¼: {scores.get('educational_value_score', 0)}/10")
        print(f"   - ç»¼åˆè¯„åˆ†: {scores.get('overall_score', 0)}/10")
        print(f"   - å†³ç­–: {'âœ… æ¥å—' if decision == 'accept' else 'ğŸ”§ éœ€è¦æ”¹è¿›'}")


class RefineAnalyzer:
    """æ”¹è¿›åˆ†æå™¨"""
    
    def __init__(self):
        print(f"âœ… RefineAnalyzeråˆå§‹åŒ–æˆåŠŸ")
    
    def analyze(self, problem: str, repi_result: dict, evaluation: dict, requirements: str = "") -> dict:
        """
        åˆ†æé¢˜ç›®é—®é¢˜å¹¶ç”Ÿæˆæ”¹è¿›å»ºè®®
        
        Args:
            problem: å½“å‰é¢˜ç›®
            repi_result: REPIéªŒè¯ç»“æœ
            evaluation: è´¨é‡è¯„ä¼°ç»“æœ
            requirements: å‡ºé¢˜è¦æ±‚
            
        Returns:
            dict: æ”¹è¿›åˆ†æç»“æœ
        """
        print(f"\nğŸ”§ [RefineAnalyzer] å¼€å§‹åˆ†ææ”¹è¿›æ–¹æ¡ˆ...")
        
        # æ„å»ºæ”¹è¿›æç¤ºè¯
        prompt = self._build_refine_prompt(problem, repi_result, evaluation, requirements)
        
        # è°ƒç”¨LLMåˆ†æ
        print(f"ğŸ”§ [RefineAnalyzer] è°ƒç”¨AIåˆ†æ...")
        response = call_llm_stream(prompt)
        
        # è§£ææ”¹è¿›ç­–ç•¥
        strategy_match = re.search(r'<improvement_strategy>(.*?)</improvement_strategy>', 
                                  response, re.DOTALL)
        strategy = strategy_match.group(1).strip() if strategy_match else ""
        
        changes_match = re.search(r'<key_changes>(.*?)</key_changes>', response, re.DOTALL)
        key_changes = changes_match.group(1).strip() if changes_match else ""
        
        steps_match = re.search(r'<expected_solve_steps>(.*?)</expected_solve_steps>', 
                               response, re.DOTALL)
        expected_steps = steps_match.group(1).strip() if steps_match else ""
        
        result = {
            'strategy': strategy,
            'key_changes': key_changes,
            'expected_steps': expected_steps,
            'raw_response': response
        }
        
        print(f"ğŸ”§ [RefineAnalyzer] æ”¹è¿›ç­–ç•¥: {strategy[:100]}...")
        
        return result
    
    def _build_refine_prompt(self, problem: str, repi_result: dict, 
                            evaluation: dict, requirements: str) -> str:
        """æ„å»ºæ”¹è¿›æç¤ºè¯"""
        
        stats = repi_result.get('statistics', {})
        scores = evaluation.get('scores', {})
        suggestions = evaluation.get('suggestions', '')
        
        # åˆ†æè§£é¢˜æƒ…å†µ
        if not repi_result.get('success'):
            solve_analysis = "REPIæ— æ³•è§£å‡ºï¼Œéœ€è¦ç®€åŒ–é¢˜ç›®æˆ–ä¿®æ­£é”™è¯¯"
        elif stats.get('total_steps', 0) < 3:
            solve_analysis = "è§£é¢˜æ­¥æ•°è¿‡å°‘ï¼Œé¢˜ç›®å¯èƒ½è¿‡äºç®€å•ï¼Œéœ€è¦å¢åŠ å¤æ‚åº¦"
        elif stats.get('total_steps', 0) > 12:
            solve_analysis = "è§£é¢˜æ­¥æ•°è¿‡å¤šï¼Œé¢˜ç›®å¯èƒ½è¿‡äºå¤æ‚ï¼Œéœ€è¦é€‚å½“ç®€åŒ–"
        elif stats.get('failed_calculations', 0) > stats.get('successful_calculations', 0):
            solve_analysis = "è®¡ç®—å¤±è´¥ç‡é«˜ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®è®¾ç½®é—®é¢˜"
        else:
            solve_analysis = "è§£é¢˜è¿‡ç¨‹åŸºæœ¬åˆç†ï¼Œä¸»è¦è¿›è¡Œç»†èŠ‚ä¼˜åŒ–"
        
        prompt = f"""
è¯·åŸºäºREPIè§£é¢˜åˆ†æå’Œè´¨é‡è¯„ä¼°æ¥æ”¹è¿›æ•°å­¦é¢˜ç›®ï¼š

## åŸé¢˜ç›®
{problem}

## å‡ºé¢˜è¦æ±‚
{requirements if requirements else "æ— ç‰¹æ®Šè¦æ±‚"}

## REPIè§£é¢˜åˆ†æ
{solve_analysis}

è¯¦ç»†è§£é¢˜æ•°æ®ï¼š
- è§£é¢˜æˆåŠŸï¼š{repi_result.get('success', False)}
- æ€»æ­¥æ•°ï¼š{stats.get('total_steps', 0)}
- æ¨ç†/è®¡ç®—æ­¥æ•°ï¼š{stats.get('reasoning_steps', 0)}/{stats.get('calculation_steps', 0)}
- è®¡ç®—æˆåŠŸ/å¤±è´¥ï¼š{stats.get('successful_calculations', 0)}/{stats.get('failed_calculations', 0)}

## è´¨é‡è¯„ä¼°ç»“æœ
- ç»¼åˆè¯„åˆ†ï¼š{scores.get('overall_score', 0)}/10
- åŸåˆ›æ€§ï¼š{scores.get('originality_score', 0)}/10
- å¯è§£æ€§ï¼š{scores.get('solvability_score', 0)}/10
- å¤æ‚åº¦ï¼š{scores.get('complexity_score', 0)}/10

## è´¨é‡è¯„ä¼°å»ºè®®
{suggestions}

## æ”¹è¿›æŒ‡å¯¼åŸåˆ™
1. æ ¹æ®REPIè§£é¢˜åˆ†æè°ƒæ•´é¢˜ç›®éš¾åº¦å’Œå¤æ‚åº¦
2. ç¡®ä¿é¢˜ç›®å¯è§£ä¸”æ­¥éª¤åˆç†ï¼ˆå»ºè®®5-12æ­¥ï¼‰
3. ä¿æŒæ•™å­¦ä»·å€¼å’Œè€ƒæŸ¥ç›®æ ‡
4. ä¼˜åŒ–é¢˜ç›®æè¿°å’Œæ•°æ®è®¾ç½®

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºæ”¹è¿›æ–¹æ¡ˆï¼š
<improvement_strategy>æ”¹è¿›ç­–ç•¥è¯´æ˜</improvement_strategy>
<key_changes>å…³é”®æ”¹åŠ¨ç‚¹</key_changes>
<expected_solve_steps>é¢„æœŸè§£é¢˜æ­¥æ•°èŒƒå›´</expected_solve_steps>
"""
        return prompt


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    evaluator = QualityEvaluator()
    
    test_problem = "å·²çŸ¥å‡½æ•° f(x) = x^2 - 4x + 3ï¼Œæ±‚å‡½æ•°çš„é›¶ç‚¹å’Œæœ€å°å€¼ã€‚"
    
    test_repi_result = {
        'success': True,
        'answer': 'é›¶ç‚¹ä¸ºx=1å’Œx=3ï¼Œæœ€å°å€¼ä¸º-1',
        'statistics': {
            'total_steps': 5,
            'reasoning_steps': 3,
            'calculation_steps': 2,
            'successful_calculations': 2,
            'failed_calculations': 0
        }
    }
    
    result = evaluator.evaluate(test_problem, test_repi_result, "é«˜ä¸­æ•°å­¦é¢˜")
    
    print("\n" + "="*80)
    print("è¯„ä¼°ç»“æœ:")
    print("="*80)
    print(f"å†³ç­–: {result['decision']}")
    print(f"è¯„åˆ†: {result['scores']}")
