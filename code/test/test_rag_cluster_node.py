#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ClusterNodeæµ‹è¯•ç”¨ä¾‹
ä½¿ç”¨rag_dataä¸‹çš„PDFæ•°æ®è¿›è¡Œæµ‹è¯•
"""
import os
import sys
import json
import tempfile
import shutil
from pathlib import Path
import csv
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "code"))
sys.path.insert(0, str(project_root / "utils"))
from code.RAG.node import ClusterNode


class TestClusterNode:
    """ClusterNodeæµ‹è¯•ç±»"""

    def __init__(self):
        self.temp_dir: str = ""
        self.test_data_dir = project_root / "data"

    def setup_test_environment(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        self.temp_dir = tempfile.mkdtemp()
        print(f"æµ‹è¯•ä¸´æ—¶ç›®å½•: {self.temp_dir}")

        # æ£€æŸ¥æµ‹è¯•æ•°æ®æ˜¯å¦å­˜åœ¨
        if not self.test_data_dir.exists():
            print(f"è­¦å‘Š: æµ‹è¯•æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.test_data_dir}")
            return False

        json_files = list(self.test_data_dir.glob("*.json"))
        if not json_files:
            print(f"è­¦å‘Š: æµ‹è¯•æ•°æ®ç›®å½•ä¸­æ²¡æœ‰JSONæ–‡ä»¶: {self.test_data_dir}")
            return False

        print(f"å‘ç°JSONæ–‡ä»¶: {[f.name for f in json_files]}")
        return True

    def cleanup_test_environment(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        if self.temp_dir and os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)
            print(f"å·²æ¸…ç†æµ‹è¯•ä¸´æ—¶ç›®å½•: {self.temp_dir}")

    def create_sample_chunks_from_json(self):
        """ä»JSONæ–‡ä»¶ä¸­åˆ›å»ºç¤ºä¾‹chunksæ•°æ®ç”¨äºæµ‹è¯•"""
        # é€‰æ‹©ä¸€ä¸ªJSONæ–‡ä»¶è¿›è¡Œæµ‹è¯•
        json_files = list(self.test_data_dir.glob("*.json"))
        if not json_files:
            raise FileNotFoundError("æ²¡æœ‰æ‰¾åˆ°JSONæ–‡ä»¶ï¼Œæ— æ³•è¿›è¡Œæµ‹è¯•")

        # ä½¿ç”¨ç¬¬ä¸€ä¸ªJSONæ–‡ä»¶
        json_file = json_files[0]
        print(f"ä½¿ç”¨æµ‹è¯•æ•°æ®æ–‡ä»¶: {json_file.name}")

        with open(json_file, "r", encoding="utf-8") as f:
            data = json.load(f)

        # æå–questionï¼ˆçº¯æ–‡æœ¬ï¼‰å­—æ®µå¹¶å»é™¤å°¾éšç©ºç™½
        sample_chunks = []
        for i, item in enumerate(data[:30]):  # å¢åŠ åˆ°30ä¸ªé—®é¢˜ä»¥è·å¾—æ›´å¥½çš„èšç±»æ•ˆæœ
            if "questionï¼ˆçº¯æ–‡æœ¬ï¼‰" in item:
                question_text = item["questionï¼ˆçº¯æ–‡æœ¬ï¼‰"].strip()
                if question_text:  # ç¡®ä¿ä¸æ˜¯ç©ºå­—ç¬¦ä¸²
                    sample_chunks.append({
                        "id": i,
                        "content": question_text
                    })

        if not sample_chunks:
            raise ValueError("ä»JSONæ–‡ä»¶ä¸­æœªæå–åˆ°æœ‰æ•ˆçš„é—®é¢˜å†…å®¹")

        chunks_path = os.path.join(self.temp_dir, "chunks.json")
        with open(chunks_path, "w", encoding="utf-8") as f:
            json.dump(sample_chunks, f, ensure_ascii=False, indent=2)

        print(f"æˆåŠŸä»JSONæ–‡ä»¶æå–äº† {len(sample_chunks)} ä¸ªé—®é¢˜ä½œä¸ºæµ‹è¯•æ•°æ®")
        return chunks_path, sample_chunks

    def visualize_clustering_hierarchy(self, cluster_data, output_path=None):
        """å¯è§†åŒ–èšç±»å±‚æ¬¡ç»“æ„"""
        if not cluster_data or not cluster_data.get('all_nodes'):
            print("æ²¡æœ‰èšç±»æ•°æ®å¯ä»¥å¯è§†åŒ–")
            return

        # æŒ‰å±‚æ¬¡å’ŒIDæ’åºæ‰€æœ‰èŠ‚ç‚¹
        all_nodes = cluster_data['all_nodes']
        all_nodes.sort(key=lambda x: (x['layer'], x['id']))

        # åˆ›å»ºå±‚æ¬¡ç»“æ„è¡¨æ ¼
        print("\n" + "=" * 100)
        print("èšç±»å±‚æ¬¡ç»“æ„å¯è§†åŒ–")
        print("=" * 100)

        # æ‰“å°è¡¨å¤´
        header_format = "{:<15} {:<8} {:<15} {:<10} {:<50}"
        print(header_format.format("èŠ‚ç‚¹ID", "å±‚çº§", "åŸå§‹ID", "å†…å®¹é•¿åº¦", "å†…å®¹é¢„è§ˆ"))
        print("-" * 100)

        # æŒ‰å±‚çº§åˆ†ç»„æ˜¾ç¤º
        current_layer = -1
        for node in all_nodes:
            if node['layer'] != current_layer:
                if current_layer >= 0:
                    print("-" * 100)
                current_layer = node['layer']
                print(f"ç¬¬{current_layer}å±‚:")

            # æˆªæ–­å†…å®¹ç”¨äºæ˜¾ç¤º
            content_preview = node['content'].replace('\n', ' ')[:47] + "..." if len(node['content']) > 50 else node[
                'content'].replace('\n', ' ')

            # æ·»åŠ ç¼©è¿›ä»¥æ˜¾ç¤ºå±‚æ¬¡
            indent = "  " * node['layer']
            node_id_display = f"{indent}{node['id']}"

            print(header_format.format(
                node_id_display,
                node['layer'],
                str(node.get('original_id', 'N/A')),
                len(node['content']),
                content_preview
            ))

        print("=" * 100)

        # åˆ›å»ºå±‚æ¬¡ç»Ÿè®¡
        layer_stats = {}
        for node in all_nodes:
            layer = node['layer']
            if layer not in layer_stats:
                layer_stats[layer] = {'count': 0, 'avg_length': 0, 'total_length': 0}
            layer_stats[layer]['count'] += 1
            layer_stats[layer]['total_length'] += len(node['content'])

        for layer in layer_stats:
            layer_stats[layer]['avg_length'] = layer_stats[layer]['total_length'] / layer_stats[layer]['count']

        print("\nå±‚æ¬¡ç»Ÿè®¡:")
        print("{:<6} {:<8} {:<12} {:<12}".format("å±‚çº§", "èŠ‚ç‚¹æ•°", "å¹³å‡é•¿åº¦", "æ€»é•¿åº¦"))
        print("-" * 45)
        for layer in sorted(layer_stats.keys()):
            stats = layer_stats[layer]
            print("{:<6} {:<8} {:<12.1f} {:<12}".format(
                layer, stats['count'], stats['avg_length'], stats['total_length']
            ))

        # å¦‚æœæŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼Œä¿å­˜åˆ°CSVæ–‡ä»¶
        if output_path:
            self.save_hierarchy_to_csv(all_nodes, layer_stats, output_path)
            print(f"\nèšç±»ç»“æœå·²ä¿å­˜åˆ°CSVæ–‡ä»¶: {output_path}")

    def save_hierarchy_to_csv(self, nodes, layer_stats, output_path):
        """å°†èšç±»å±‚æ¬¡ç»“æ„ä¿å­˜åˆ°CSVæ–‡ä»¶"""
        with open(output_path, 'w', encoding='utf-8', newline='') as csvfile:
            writer = csv.writer(csvfile)

            # å†™å…¥èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯
            writer.writerow(['èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯'])
            writer.writerow(['èŠ‚ç‚¹ID', 'å±‚çº§', 'åŸå§‹ID', 'å†…å®¹é•¿åº¦', 'å­èŠ‚ç‚¹', 'å†…å®¹'])

            for node in nodes:
                children_str = ', '.join(node.get('children', []))
                writer.writerow([
                    node['id'],
                    node['layer'],
                    node.get('original_id', 'N/A'),
                    len(node['content']),
                    children_str,
                    node['content']
                ])

            # å†™å…¥å±‚æ¬¡ç»Ÿè®¡
            writer.writerow([])
            writer.writerow(['å±‚æ¬¡ç»Ÿè®¡'])
            writer.writerow(['å±‚çº§', 'èŠ‚ç‚¹æ•°', 'å¹³å‡å†…å®¹é•¿åº¦', 'æ€»å†…å®¹é•¿åº¦'])

            for layer in sorted(layer_stats.keys()):
                stats = layer_stats[layer]
                writer.writerow([layer, stats['count'], f"{stats['avg_length']:.1f}", stats['total_length']])

    def visualize_parent_child_relationships(self, cluster_data):
        """å¯è§†åŒ–çˆ¶å­å…³ç³»æ ‘"""
        if not cluster_data or not cluster_data.get('all_nodes'):
            return

        print("\n" + "=" * 80)
        print("çˆ¶å­å…³ç³»æ ‘ç»“æ„")
        print("=" * 80)

        # åˆ›å»ºçˆ¶å­å…³ç³»æ˜ å°„
        nodes_by_id = {node['id']: node for node in cluster_data['all_nodes']}
        children_map = {}

        for node in cluster_data['all_nodes']:
            if node.get('children'):
                children_map[node['id']] = node['children']

        # æ‰¾åˆ°æ ¹èŠ‚ç‚¹ï¼ˆæ²¡æœ‰è¢«ä»»ä½•èŠ‚ç‚¹å¼•ç”¨ä½œä¸ºå­èŠ‚ç‚¹çš„èŠ‚ç‚¹ï¼‰
        all_children = set()
        for children_list in children_map.values():
            all_children.update(children_list)

        root_nodes = [node for node in cluster_data['all_nodes']
                      if node['id'] not in all_children and node.get('children')]

        def print_tree(node_id, level=0, prefix=""):
            if node_id not in nodes_by_id:
                return

            node = nodes_by_id[node_id]
            content_preview = node['content'][:40] + "..." if len(node['content']) > 40 else node['content']
            content_preview = content_preview.replace('\n', ' ')

            print(f"{prefix}â”œâ”€â”€ {node_id} (L{node['layer']}) [{len(node['content'])}å­—ç¬¦] {content_preview}")

            # æ‰“å°å­èŠ‚ç‚¹
            children = node.get('children', [])
            for i, child_id in enumerate(children):
                is_last = i == len(children) - 1
                child_prefix = prefix + ("    " if is_last else "â”‚   ")
                print_tree(child_id, level + 1, child_prefix)

        # æ‰“å°æ‰€æœ‰æ ¹èŠ‚ç‚¹çš„æ ‘
        if root_nodes:
            for root in root_nodes:
                print(f"\næ ‘ {root['id']}:")
                print_tree(root['id'])
        else:
            print("æœªæ‰¾åˆ°æ˜ç¡®çš„æ ¹èŠ‚ç‚¹ï¼Œæ˜¾ç¤ºæ‰€æœ‰èŠ‚ç‚¹:")
            layer_0_nodes = [node for node in cluster_data['all_nodes'] if node['layer'] == 0]
            for node in layer_0_nodes[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªå¶å­èŠ‚ç‚¹
                print(f"â”œâ”€â”€ {node['id']} (L{node['layer']}) [{len(node['content'])}å­—ç¬¦] {node['content'][:40]}...")

        print("=" * 80)

    def test_cluster_node_basic(self):
        """æµ‹è¯•ClusterNodeåŸºæœ¬åŠŸèƒ½"""
        print("\n=== æµ‹è¯•ClusterNodeåŸºæœ¬åŠŸèƒ½ ===")

        # åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨JSONæ–‡ä»¶æ•°æ®ï¼‰
        chunks_path, sample_chunks = self.create_sample_chunks_from_json()
        cluster_db_path = os.path.join(self.temp_dir, "cluster.json")

        # è®¾ç½®sharedå‚æ•°
        shared = {
            "chunks_path": chunks_path,
            "cluster_db_path": cluster_db_path
        }

        # åˆ›å»ºClusterNodeå®ä¾‹ï¼Œä½¿ç”¨ä¼˜åŒ–çš„å‚æ•°ä¿ƒè¿›çœŸå®çš„å¤šå±‚èšç±»
        cluster_node = ClusterNode(
            max_clusters=3,  # é€‚ä¸­çš„èšç±»æ•°ï¼Œä¾¿äºå½¢æˆå±‚æ¬¡
            min_cluster_size=2,  # ä¿æŒæœ€å°èšç±»å¤§å°
            max_layers=5,  # å¢åŠ æœ€å¤§å±‚æ•°
            summary_threshold=50  # é™ä½æ‘˜è¦é˜ˆå€¼ï¼Œæ›´å®¹æ˜“è§¦å‘èšç±»
        )

        try:
            # è¿è¡ŒClusterNode
            result = cluster_node.run(shared)
            print(f"ClusterNodeè¿è¡Œç»“æœ: {result}")

            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
            if os.path.exists(cluster_db_path):
                with open(cluster_db_path, "r", encoding="utf-8") as f:
                    cluster_data = json.load(f)

                print(f"èšç±»ç»“æœåŒ…å« {len(cluster_data.get('all_nodes', []))} ä¸ªèŠ‚ç‚¹")

                # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
                for node in cluster_data.get('all_nodes', [])[:5]:
                    print(f"èŠ‚ç‚¹ {node['id']}: å±‚çº§{node['layer']}, å†…å®¹é•¿åº¦{len(node['content'])}")

                # å¯è§†åŒ–èšç±»å±‚æ¬¡ç»“æ„
                csv_output_path = os.path.join(self.temp_dir, "clustering_hierarchy_basic.csv")
                # ä¹Ÿä¿å­˜åˆ°é¡¹ç›®è¾“å‡ºç›®å½•
                permanent_csv_path = project_root / "output_data" / "clustering_hierarchy_basic.csv"
                os.makedirs(permanent_csv_path.parent, exist_ok=True)
                self.visualize_clustering_hierarchy(cluster_data, csv_output_path)

                # å¤åˆ¶åˆ°æ°¸ä¹…ä½ç½®
                if os.path.exists(csv_output_path):
                    shutil.copy2(csv_output_path, permanent_csv_path)
                    print(f"èšç±»ç»“æœå·²å¤åˆ¶åˆ°æ°¸ä¹…ä½ç½®: {permanent_csv_path}")

                self.visualize_parent_child_relationships(cluster_data)

                return True
            else:
                print(f"é”™è¯¯: èšç±»ç»“æœæ–‡ä»¶æœªç”Ÿæˆ: {cluster_db_path}")
                return False

        except Exception as e:
            print(f"ClusterNodeæµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    def test_cluster_node_with_real_json_data(self):
        """ä½¿ç”¨çœŸå®JSONæ•°æ®æµ‹è¯•ClusterNode"""
        print("\n=== ä½¿ç”¨çœŸå®JSONæ•°æ®æµ‹è¯•ClusterNode ===")

        if not self.test_data_dir.exists():
            print("è·³è¿‡çœŸå®æ•°æ®æµ‹è¯•ï¼šæµ‹è¯•æ•°æ®ç›®å½•ä¸å­˜åœ¨")
            return True

        json_files = list(self.test_data_dir.glob("*.json"))
        if not json_files:
            print("è·³è¿‡çœŸå®æ•°æ®æµ‹è¯•ï¼šæ²¡æœ‰æ‰¾åˆ°JSONæ–‡ä»¶")
            return True

        try:
            # é€‰æ‹©ä¸€ä¸ªè¾ƒå¤§çš„JSONæ–‡ä»¶è¿›è¡Œæµ‹è¯•
            test_json_file = None
            for json_file in json_files:
                # if json_file.name in ["é«˜è€ƒéš¾é¢˜.json", "ç²¾é€‰é¢˜.json", "25é¢˜.json"]:  # ä¼˜å…ˆé€‰æ‹©è¿™äº›æ–‡ä»¶
                # if json_file.name in ["é«˜è€ƒéš¾é¢˜.json"]:  # ä¼˜å…ˆé€‰æ‹©è¿™äº›æ–‡ä»¶
                test_json_file = json_file
                break

            if not test_json_file:
                test_json_file = json_files[0]  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¼˜å…ˆæ–‡ä»¶ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª

            print(f"ä½¿ç”¨JSONæ–‡ä»¶è¿›è¡Œæµ‹è¯•: {test_json_file.name}")

            # è¯»å–JSONæ•°æ®å¹¶æå–é—®é¢˜æ–‡æœ¬
            with open(test_json_file, "r", encoding="utf-8") as f:
                data = json.load(f)

            # åˆ›å»ºchunksæ•°æ®
            chunks = []
            for i, item in enumerate(data[:30]):  # å¢åŠ æ•°é‡ä»¥è·å¾—æ›´å¥½çš„èšç±»å±‚æ¬¡
                if "questionï¼ˆçº¯æ–‡æœ¬ï¼‰" in item:
                    question_text = item["questionï¼ˆçº¯æ–‡æœ¬ï¼‰"].strip()
                    if question_text:
                        chunks.append({
                            "id": i,
                            "content": question_text
                        })

            if len(chunks) < 5:
                print(f"è­¦å‘Š: æœ‰æ•ˆé—®é¢˜æ•°é‡å¤ªå°‘({len(chunks)})ï¼Œè·³è¿‡æµ‹è¯•")
                return True

            # è®¾ç½®æ–‡ä»¶è·¯å¾„
            chunks_path = os.path.join(self.temp_dir, "real_chunks.json")
            cluster_db_path = os.path.join(self.temp_dir, "real_cluster.json")

            # ä¿å­˜chunksæ•°æ®
            with open(chunks_path, "w", encoding="utf-8") as f:
                json.dump(chunks, f, ensure_ascii=False, indent=2)

            # è®¾ç½®sharedå‚æ•°
            shared = {
                "chunks_path": chunks_path,
                "cluster_db_path": cluster_db_path
            }

            # åˆ›å»ºå¹¶è¿è¡ŒClusterNodeï¼Œä½¿ç”¨ä¼˜åŒ–å‚æ•°ä¿ƒè¿›çœŸå®å¤šå±‚èšç±»
            cluster_node = ClusterNode(
                max_clusters=3,  # é€‚ä¸­çš„èšç±»æ•°
                min_cluster_size=2,  # æœ€å°èšç±»å¤§å°
                max_layers=5,  # æ›´å¤šå±‚æ•°
                summary_threshold=50  # é™ä½æ‘˜è¦é˜ˆå€¼ï¼Œæ›´å®¹æ˜“å½¢æˆå±‚æ¬¡
            )

            print(f"å¼€å§‹å¤„ç† {len(chunks)} ä¸ªæ•°å­¦é—®é¢˜...")
            result = cluster_node.run(shared)
            print(f"JSONæ•°æ®å¤„ç†æµç¨‹å®Œæˆ: {result}")

            # æ£€æŸ¥ç»“æœ
            if os.path.exists(cluster_db_path):
                with open(cluster_db_path, "r", encoding="utf-8") as f:
                    cluster_data = json.load(f)

                print(f"ä»çœŸå®JSONæ•°æ®ç”Ÿæˆäº† {len(cluster_data.get('all_nodes', []))} ä¸ªèšç±»èŠ‚ç‚¹")

                # æ˜¾ç¤ºå„å±‚çº§çš„èŠ‚ç‚¹æ•°é‡
                layer_counts = {}
                for node in cluster_data.get('all_nodes', []):
                    layer = node['layer']
                    layer_counts[layer] = layer_counts.get(layer, 0) + 1

                for layer, count in sorted(layer_counts.items()):
                    print(f"ç¬¬{layer}å±‚: {count} ä¸ªèŠ‚ç‚¹")

                # å±•ç¤ºä¸€äº›èšç±»å†…å®¹ç¤ºä¾‹
                print("\nèšç±»å†…å®¹ç¤ºä¾‹:")
                for node in cluster_data.get('all_nodes', [])[:3]:
                    content_preview = node['content'][:100] + "..." if len(node['content']) > 100 else node['content']
                    print(f"- èŠ‚ç‚¹{node['id']} (ç¬¬{node['layer']}å±‚): {content_preview}")

                # å¯è§†åŒ–èšç±»å±‚æ¬¡ç»“æ„
                csv_output_path = os.path.join(self.temp_dir, f"clustering_hierarchy_{test_json_file.stem}.csv")
                # ä¹Ÿä¿å­˜åˆ°é¡¹ç›®è¾“å‡ºç›®å½•
                permanent_csv_path = project_root / "output_data" / f"clustering_hierarchy_{test_json_file.stem}.csv"
                os.makedirs(permanent_csv_path.parent, exist_ok=True)
                self.visualize_clustering_hierarchy(cluster_data, csv_output_path)

                # å¤åˆ¶åˆ°æ°¸ä¹…ä½ç½®
                if os.path.exists(csv_output_path):
                    shutil.copy2(csv_output_path, permanent_csv_path)
                    print(f"èšç±»ç»“æœå·²å¤åˆ¶åˆ°æ°¸ä¹…ä½ç½®: {permanent_csv_path}")

                self.visualize_parent_child_relationships(cluster_data)

                return True
            else:
                print("é”™è¯¯: èšç±»ç»“æœæ–‡ä»¶æœªç”Ÿæˆ")
                return False

        except Exception as e:
            print(f"çœŸå®JSONæ•°æ®æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    def test_cluster_node_edge_cases(self):
        """æµ‹è¯•ClusterNodeè¾¹ç•Œæƒ…å†µ"""
        print("\n=== æµ‹è¯•ClusterNodeè¾¹ç•Œæƒ…å†µ ===")

        # æµ‹è¯•ç©ºè¾“å…¥
        print("æµ‹è¯•ç©ºè¾“å…¥...")
        empty_chunks_path = os.path.join(self.temp_dir, "empty_chunks.json")
        with open(empty_chunks_path, "w", encoding="utf-8") as f:
            json.dump([], f)

        cluster_db_path = os.path.join(self.temp_dir, "empty_cluster.json")
        shared = {
            "chunks_path": empty_chunks_path,
            "cluster_db_path": cluster_db_path
        }

        cluster_node = ClusterNode()
        try:
            result = cluster_node.run(shared)
            print(f"ç©ºè¾“å…¥æµ‹è¯•é€šè¿‡: {result}")
        except Exception as e:
            print(f"ç©ºè¾“å…¥æµ‹è¯•å¤±è´¥: {e}")
            return False

        # æµ‹è¯•å•ä¸ªæ–‡æ¡£ - ä½¿ç”¨çœŸå®çš„æ•°å­¦é—®é¢˜
        print("æµ‹è¯•å•ä¸ªæ–‡æ¡£...")

        # å°è¯•ä»JSONæ–‡ä»¶è·å–ä¸€ä¸ªçœŸå®çš„æ•°å­¦é—®é¢˜
        json_files = list(self.test_data_dir.glob("*.json"))
        single_content = "è¿™æ˜¯ä¸€ä¸ªå•ç‹¬çš„æ•°å­¦é—®é¢˜ç”¨äºæµ‹è¯•èšç±»åŠŸèƒ½ã€‚"

        if json_files:
            try:
                with open(json_files[0], "r", encoding="utf-8") as f:
                    data = json.load(f)
                if data and isinstance(data, list) and len(data) > 0:
                    first_item = data[0]
                    if "questionï¼ˆçº¯æ–‡æœ¬ï¼‰" in first_item:
                        question_text = first_item["questionï¼ˆçº¯æ–‡æœ¬ï¼‰"].strip()
                        if question_text:
                            single_content = question_text
            except Exception:
                pass  # ä½¿ç”¨é»˜è®¤å†…å®¹

        single_chunk = [{"id": 0, "content": single_content}]
        single_chunks_path = os.path.join(self.temp_dir, "single_chunks.json")
        with open(single_chunks_path, "w", encoding="utf-8") as f:
            json.dump(single_chunk, f, ensure_ascii=False)

        single_cluster_db_path = os.path.join(self.temp_dir, "single_cluster.json")
        shared = {
            "chunks_path": single_chunks_path,
            "cluster_db_path": single_cluster_db_path
        }

        try:
            result = cluster_node.run(shared)
            print(f"å•æ–‡æ¡£æµ‹è¯•é€šè¿‡: {result}")
            return True
        except Exception as e:
            print(f"å•æ–‡æ¡£æµ‹è¯•å¤±è´¥: {e}")
            return False

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("å¼€å§‹ClusterNodeæµ‹è¯•...")

        if not self.setup_test_environment():
            print("æµ‹è¯•ç¯å¢ƒè®¾ç½®å¤±è´¥")
            return False

        try:
            test_results = []

            # è¿è¡Œå„é¡¹æµ‹è¯•
            test_results.append(self.test_cluster_node_basic())
            test_results.append(self.test_cluster_node_edge_cases())
            test_results.append(self.test_cluster_node_with_real_json_data())

            # æ±‡æ€»ç»“æœ
            passed = sum(test_results)
            total = len(test_results)

            print("\n=== æµ‹è¯•ç»“æœæ±‡æ€» ===")
            print(f"é€šè¿‡: {passed}/{total}")

            if passed == total:
                print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
                return True
            else:
                print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
                return False

        finally:
            self.cleanup_test_environment()


def main():
    """ä¸»å‡½æ•°"""
    tester = TestClusterNode()
    success = tester.run_all_tests()

    if success:
        print("\nğŸ‰ ClusterNodeæµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
    else:
        print("\nğŸ’¥ ClusterNodeæµ‹è¯•å­˜åœ¨å¤±è´¥é¡¹ï¼Œè¯·æ£€æŸ¥è¾“å‡º")

    return 0 if success else 1


if __name__ == "__main__":
    exit(main())
