import pocketflow as pf
import openai  # éµå¾ªè§„èŒƒï¼Œå¯¼å…¥openai
import os
import json
import csv
import sys
import time
import threading
import concurrent.futures
from dotenv import load_dotenv

# ==============================================================================
# 0. è·¯å¾„å’Œç¯å¢ƒè®¾ç½®
# ç¡®ä¿æ— è®ºä»å“ªé‡Œæ‰§è¡Œï¼Œéƒ½èƒ½æ­£ç¡®æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•å’Œæ¨¡å—
# ==============================================================================
try:
    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
except NameError:
    SCRIPT_DIR = os.getcwd()

# å‡è®¾æ­¤è„šæœ¬ä½äºé¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ 'scripts' æ–‡ä»¶å¤¹ä¸­
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '../..'))

if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

# ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½ .env æ–‡ä»¶
load_dotenv(os.path.join(PROJECT_ROOT, '.env'))

# ==============================================================================
# 1. é…ç½®åŠ è½½ä¸OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–
# ==============================================================================
MODEL_NAME = os.getenv("MODEL_NAME", "default_model")
MAX_RETRY = int(os.getenv("MAX_RETRY", "3"))
FILENAME = os.getenv("FILE_NAME", "math_test")  # é»˜è®¤æ•°æ®é›†æ–‡ä»¶å
MAX_WORKERS = int(os.getenv("CONCURRENCY_LIMIT", "4"))  # ä»ç¯å¢ƒå˜é‡é…ç½®å¹¶å‘æ•°

# ==============================================================================
# 2. èŠ‚ç‚¹ä¸æµç¨‹å®šä¹‰
# å¯¼å…¥ä½ å·²æœ‰çš„èŠ‚ç‚¹ï¼Œå¹¶æ·»åŠ è¯„ä¼°èŠ‚ç‚¹
# ==============================================================================
# ä»ä½ çš„é¡¹ç›®ä¸­å¯¼å…¥èŠ‚ç‚¹å®šä¹‰
from code.DeRePI.node import DecomposerNode, ReNode, PINode, AnswerNode
from utils.llm import call_llm_stream  # å‡è®¾è¿™ä¸ªå‡½æ•°å­˜åœ¨ä¸”èƒ½ç”¨
from utils.prompt_templates import REPI_EVALUATION_NODE_PROMPT  # éœ€è¦è¯„ä¼°prompt


class EvaluationNode(pf.Node):
    """
    è¯„ä¼°èŠ‚ç‚¹ï¼Œç”¨äºæ¯”è¾ƒæ¨¡å‹è¾“å‡ºå’Œæ ‡å‡†ç­”æ¡ˆã€‚
    """

    def prep(self, shared):
        # ä»sharedä¸­è·å–é—®é¢˜ã€æ¨¡å‹ç­”æ¡ˆå’ŒçœŸå®ç­”æ¡ˆ
        return {
            "model_answer": shared.get("answer", "NO_ANSWER_FOUND"),  # DeRePIçš„æœ€ç»ˆç­”æ¡ˆåœ¨'answer'å­—æ®µ
            "ground_truth": shared.get("truth", "NO_TRUTH_PROVIDED"),
            "question": shared.get("question", "NO_QUESTION_FOUND")
        }

    def exec(self, prep_res):
        # ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œè¯„ä¼°æ‰“åˆ†
        eval_prompt = REPI_EVALUATION_NODE_PROMPT.format(**prep_res)
        # å‡è®¾call_llm_streamå†…éƒ¨å¤„ç†äº†APIè°ƒç”¨é€»è¾‘
        response = call_llm_stream(eval_prompt)  # å°†clientä¼ é€’è¿›å»
        return response

    def post(self, shared, prep_res, exec_res):
        # å°†æœ€ç»ˆè¯„ä¼°ç»“æœå­˜å…¥shared
        shared['final_result'] = exec_res


def create_derepi_test_pipeline():
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºä»åˆ†è§£ã€æ‰§è¡Œåˆ°æœ€ç»ˆè¯„ä¼°çš„å®Œæ•´DeRePIæµ‹è¯•æµç¨‹ã€‚
    è¿™ç¡®ä¿äº†æ¯ä¸ªå¹¶å‘çº¿ç¨‹éƒ½ä½¿ç”¨è‡ªå·±ç‹¬ç«‹çš„æµç¨‹å®ä¾‹ï¼Œé¿å…çŠ¶æ€å†²çªã€‚
    """
    # 1. å®ä¾‹åŒ–æ‰€æœ‰éœ€è¦çš„èŠ‚ç‚¹
    decomposer_node = DecomposerNode()
    re_node = ReNode(max_retries=MAX_RETRY)
    pi_node = PINode()
    answer_node = AnswerNode()
    evaluation_node = EvaluationNode()  # æ–°å¢è¯„ä¼°èŠ‚ç‚¹

    # 2. å®šä¹‰å­æµç¨‹ï¼šReNode å’Œ PINode ä¹‹é—´çš„æ¨ç†-è®¡ç®—å¾ªç¯
    re_node.next(pi_node, action="calculate")
    pi_node.next(re_node, action="feedback")

    # ReNodeåœ¨å­ä»»åŠ¡å®Œæˆåä¼šè¿”å› "sub_task_complete"ï¼Œ
    # è¿™å°†ç»“æŸå½“å‰å­ä»»åŠ¡åœ¨BatchFlowä¸­çš„æ‰§è¡Œã€‚

    # 3. åˆ›å»ºæ‰¹å¤„ç†æµç¨‹ (BatchFlow) æ¥æ‰§è¡Œæ‰€æœ‰å­ä»»åŠ¡
    task_executor_flow = pf.BatchFlow()

    # ç»‘å®šæ•°æ®å‡†å¤‡æ–¹æ³•åˆ°BatchFlowå®ä¾‹
    def task_executor_prep(self, shared):
        steps = shared.get('steps', [])
        # å½“åˆ†è§£å‡º0ä¸ªæ­¥éª¤æ—¶ï¼Œstepså¯èƒ½æ˜¯['end']ï¼Œæˆ‘ä»¬éœ€è¦è¿‡æ»¤æ‰
        valid_steps = [s for s in steps if s.lower() != 'end']
        print(f"ğŸ”„ [BatchFlow] å‡†å¤‡æ‰§è¡Œ {len(valid_steps)} ä¸ªå­ä»»åŠ¡...")
        return [{'task': step} for step in valid_steps]

    task_executor_flow.prep = task_executor_prep.__get__(task_executor_flow, pf.BatchFlow)

    # è®¾ç½®æ‰¹å¤„ç†æµç¨‹çš„èµ·ç‚¹
    task_executor_flow.start(re_node)

    # 4. æ„å»ºä¸»æµç¨‹ (Main Flow)
    main_flow = pf.Flow()
    main_flow.start(decomposer_node)

    # 5. è¿æ¥ä¸»æµç¨‹çš„å„ä¸ªé˜¶æ®µ
    # åˆ†è§£å™¨å®Œæˆåï¼Œå¦‚æœéœ€è¦æ‰§è¡Œè®¡åˆ’ï¼Œåˆ™å¯åŠ¨æ‰¹å¤„ç†æµç¨‹
    decomposer_node.next(task_executor_flow, action="execute_plan")

    # å¦‚æœåˆ†è§£å™¨ç›´æ¥ç»“æŸ(ä¾‹å¦‚é—®é¢˜å¤ªç®€å•)ï¼Œåˆ™ç›´æ¥è·³åˆ°å›ç­”èŠ‚ç‚¹
    # æ³¨æ„ï¼šä½ çš„DecomposerNodeå®ç°ä¸­ï¼Œå¦‚æœæ²¡æœ‰æ­¥éª¤ä¼šè¿”å›'end'ï¼Œä½†æ²¡æœ‰å®šä¹‰endçš„æµå‘
    # æˆ‘ä»¬è¿™é‡Œå°†å…¶å¯¼å‘AnswerNodeï¼Œè®©å®ƒå°è¯•åŸºäºç°æœ‰ä¿¡æ¯å›ç­”
    decomposer_node.next(answer_node, action="end")

    # æ‰¹å¤„ç†æµç¨‹å®Œæˆåï¼Œæµå‘å›ç­”èŠ‚ç‚¹è¿›è¡Œç­”æ¡ˆæ•´åˆ
    task_executor_flow.next(answer_node)

    # å›ç­”èŠ‚ç‚¹å®Œæˆåï¼Œæµå‘è¯„ä¼°èŠ‚ç‚¹è¿›è¡Œæœ€ç»ˆè¯„ä¼°
    answer_node.next(evaluation_node)

    return main_flow


def process_item_and_write_csv(data_item, f_csv, csv_writer, csv_lock):
    """
    å¤„ç†å•ä¸ªæ•°æ®é¡¹ï¼Œå¹¶çº¿ç¨‹å®‰å…¨åœ°å°†å…¶ç»“æœç«‹å³å†™å…¥å¹¶åˆ·æ–°åˆ°CSVæ–‡ä»¶ã€‚
    è¿™æ˜¯å¹¶å‘å¤„ç†çš„æ ¸å¿ƒå·¥ä½œå‡½æ•°ã€‚
    """
    thread_id = threading.get_ident()
    current_id = str(data_item.get("id", f"no-id-{int(time.time())}"))

    print(f"[{thread_id}] âš™ï¸  å¼€å§‹å¤„ç† ID: {current_id}...")

    # ä¸ºå½“å‰ä»»åŠ¡åˆ›å»ºä¸€ä¸ªå…¨æ–°çš„ã€ç‹¬ç«‹çš„æµç¨‹å®ä¾‹
    test_pipeline = create_derepi_test_pipeline()

    # åˆå§‹åŒ–å½“å‰ä»»åŠ¡çš„å…±äº«çŠ¶æ€
    shared = {
        "id": current_id,
        "question": data_item.get("questionï¼ˆçº¯æ–‡æœ¬ï¼‰", data_item.get("question", "N/A")),
        "truth": str(data_item.get('ground_truth', "N/A")),
        'responses': [],
        'actions': [],
        'codes': [],
        'calculation_results': [],
        'node_call_counts': {},
        'answer': ''
    }

    try:
        # è¿è¡Œå®Œæ•´çš„æµ‹è¯•æµç¨‹
        test_pipeline.run(shared)
        result_row = [
            current_id,
            shared.get("question"),
            shared.get("answer", "ANSWER_ERROR"),  # æœ€ç»ˆç­”æ¡ˆ
            shared.get("truth"),
            shared.get("final_result", "EVAL_ERROR")  # è¯„ä¼°ç»“æœ
        ]
        print(f"[{thread_id}] âœ… ID: {current_id} å¤„ç†æˆåŠŸã€‚")
    except Exception as e:
        print(f"[{thread_id}] âŒ ID: {current_id} å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        result_row = [
            current_id,
            shared.get("question"),
            'FATAL_ERROR',
            shared.get("truth"),
            str(e)
        ]

    # --- çº¿ç¨‹å®‰å…¨åœ°ã€ç«‹å³å†™å…¥å¹¶åˆ·æ–°åˆ°ç£ç›˜ï¼ˆå®ç°ä¸­æ–­å®‰å…¨çš„å…³é”®ï¼‰ ---
    with csv_lock:
        csv_writer.writerow(result_row)
        f_csv.flush()  # å¼ºåˆ¶å°†ç¼“å†²åŒºå†…å®¹å†™å…¥ç£ç›˜


# ==============================================================================
# 5. å¹¶å‘æ‰¹å¤„ç†ä¸»ç¨‹åº
# ==============================================================================
if __name__ == '__main__':
    start_time = time.time()
    print("ğŸš€ å¼€å§‹DeRePIæ™ºèƒ½ä½“è‡ªåŠ¨åŒ–å¹¶å‘æµ‹è¯•æµç¨‹...")

    # --- æ–‡ä»¶è·¯å¾„è®¾ç½® ---
    output_dir = os.path.join(PROJECT_ROOT, 'output_data')
    os.makedirs(output_dir, exist_ok=True)
    output_csv_filename = os.path.join(output_dir, f'{FILENAME}_DeRePI_{MODEL_NAME}_å¯¹æ¯”ç»“æœ.csv')

    print(f"ğŸ“‚ æ•°æ®é›†è·¯å¾„: {os.path.join(PROJECT_ROOT, 'data', f'{FILENAME}.json')}")
    print(f"ğŸ“„ CSVè¾“å‡ºè·¯å¾„: {output_csv_filename}")
    print(f"ğŸ¤– æ¨¡å‹: {MODEL_NAME}, âš™ï¸ æœ€å¤§å¹¶å‘æ•°: {MAX_WORKERS}")

    # --- æ–­ç‚¹ç»­ä¼ é€»è¾‘ ---
    processed_ids = set()
    if os.path.exists(output_csv_filename):
        print("ğŸ”„ æ£€æµ‹åˆ°è¾“å‡ºæ–‡ä»¶ï¼Œæ­£åœ¨æ¢å¤è¿›åº¦...")
        try:
            with open(output_csv_filename, 'r', encoding='utf-8-sig') as f_read:
                reader = csv.reader(f_read)
                header = next(reader, None)
                if header:
                    for row in reader:
                        if row and row[0]:
                            processed_ids.add(row[0])
            print(f"âœ… æˆåŠŸæ¢å¤ï¼å·²å¤„ç† {len(processed_ids)} ä¸ªæ¡ç›®ã€‚")
        except Exception as e:
            print(f"âš ï¸ è¯»å–CSVæ–‡ä»¶æ¢å¤è¿›åº¦æ—¶å‡ºé”™: {e}ã€‚å°†é‡æ–°å¼€å§‹ã€‚")
            processed_ids = set()

    # --- åŠ è½½å¹¶è¿‡æ»¤æ•°æ®é›† ---
    try:
        with open(os.path.join(PROJECT_ROOT, 'data', f'{FILENAME}.json'), 'r', encoding='utf-8') as f:
            all_datasets = json.load(f)
        tasks_to_run = [
            data for i, data in enumerate(all_datasets)
            if str(data.get("id", f"no-id-{i}")) not in processed_ids
        ]
        print(f"ğŸ“š æ•°æ®é›†åŠ è½½æˆåŠŸã€‚å…± {len(all_datasets)} æ¡ï¼Œéœ€å¤„ç† {len(tasks_to_run)} æ¡æ–°ä»»åŠ¡ã€‚")
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®é›†æ–‡ä»¶ at {os.path.join(PROJECT_ROOT, 'data', f'{FILENAME}.json')}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        sys.exit(1)

    if not tasks_to_run:
        print("ğŸ‰ æ— æ–°ä»»åŠ¡éœ€è¦å¤„ç†ã€‚ç¨‹åºç»“æŸã€‚")
        sys.exit(0)

    # --- å¹¶å‘æ‰§è¡Œä¸å†™å…¥ ---
    with open(output_csv_filename, 'a', encoding='utf-8-sig', newline='') as f_csv:
        writer = csv.writer(f_csv)

        # å¦‚æœæ–‡ä»¶æ˜¯æ–°å»ºçš„ï¼Œå†™å…¥è¡¨å¤´
        if not processed_ids:
            writer.writerow(['id', 'problem', 'model_answer', 'truth', 'final_result'])
            f_csv.flush()

        # åˆ›å»ºä¸€ä¸ªé”æ¥ä¿æŠ¤å¯¹CSVæ–‡ä»¶çš„å†™å…¥æ“ä½œ
        csv_writer_lock = threading.Lock()

        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # æäº¤æ‰€æœ‰å¾…å¤„ç†ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
            futures = [
                executor.submit(process_item_and_write_csv, data, f_csv, writer, csv_writer_lock)
                for data in tasks_to_run
            ]

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future in concurrent.futures.as_completed(futures):
                try:
                    future.result()  # è·å–ç»“æœï¼Œå¦‚æœä»»åŠ¡ä¸­å‘ç”Ÿå¼‚å¸¸ï¼Œè¿™é‡Œä¼šé‡æ–°æŠ›å‡º
                except Exception as exc:
                    print(f"ğŸ’¥ [ä¸»çº¿ç¨‹] ä¸€ä¸ªå·¥ä½œçº¿ç¨‹å¥”æºƒ: {exc}")

    end_time = time.time()
    print("-" * 60)
    print(f"ğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’ã€‚")
    print(f"ğŸ“„ ç»“æœå·²å…¨éƒ¨ä¿å­˜è‡³: '{output_csv_filename}'")
