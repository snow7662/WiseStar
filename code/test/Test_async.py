import pocketflow as pf
import os
import json
import csv
import sys
import time
import threading  # å¯¼å…¥threadingæ¨¡å—
import concurrent.futures  # å¯¼å…¥å¹¶å‘åº“
from dotenv import load_dotenv
# æ·»åŠ å·¥ä½œæµ
from Work_Flow import get_nodes, select_flow

# ==============================================================================
# 0. è·¯å¾„å’Œç¯å¢ƒè®¾ç½® (ä¸åŸç‰ˆç›¸åŒ)
# ==============================================================================
# é€šè¿‡è®¡ç®—è„šæœ¬çš„ç»å¯¹è·¯å¾„æ¥ç¡®ä¿æ— è®ºä»å“ªé‡Œæ‰§è¡Œï¼Œéƒ½èƒ½æ­£ç¡®æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
try:
    # __file__ åœ¨æ­£å¸¸ Python æ‰§è¡Œæ—¶å¯ç”¨
    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
except NameError:
    # åœ¨äº¤äº’å¼ç¯å¢ƒï¼ˆå¦‚Jupyterï¼‰ä¸­ï¼Œ__file__ ä¸å­˜åœ¨ï¼Œä½¿ç”¨å½“å‰å·¥ä½œç›®å½•
    SCRIPT_DIR = os.getcwd()

PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..'))

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„ï¼Œä»¥ä¾¿èƒ½æˆåŠŸå¯¼å…¥ utils æ¨¡å—
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

# ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½ .env æ–‡ä»¶
load_dotenv(os.path.join(PROJECT_ROOT, '.env'))


# ==============================================================================
# 1. é…ç½®åŠ è½½ (æ–°å¢å¹¶å‘ç›¸å…³é…ç½®)
# ==============================================================================
MODEL_NAME = os.getenv("MODEL_NAME", "default_model")
MAX_RETRY = int(os.getenv("MAX_RETRY", "3"))
FILENAME = os.getenv("FILE_NAME")
# æ–°å¢ï¼šä»ç¯å¢ƒå˜é‡é…ç½®æœ€å¤§å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸º4
MAX_WORKERS = int(os.getenv("CONCURRENCY_LIMIT", "4"))


# åŠ¨æ€å¯¼å…¥ç›¸åº”çš„èŠ‚ç‚¹è¿›è¡Œå¤„ç†
# ==============================================================================
# 2. èŠ‚ç‚¹ä¸æµç¨‹å®šä¹‰ (ä¸åŸç‰ˆåŸºæœ¬ç›¸åŒ)
# ==============================================================================


def process_item_and_write_csv(data_item, f_csv, csv_writer, csv_lock,module_type):
    """
    å¤„ç†å•ä¸ªæ•°æ®é¡¹ï¼Œå¹¶çº¿ç¨‹å®‰å…¨åœ°å°†å…¶ç»“æœç«‹å³å†™å…¥å¹¶åˆ·æ–°åˆ°CSVæ–‡ä»¶ã€‚

    Args:
        data_item (dict): æ•°æ®é›†ä¸­çš„ä¸€ä¸ªå…ƒç´ ã€‚
        f_csv (file object): æ–‡ä»¶å¥æŸ„ï¼Œç”¨äºè°ƒç”¨ flush()ã€‚
        csv_writer: csv.writer å¯¹è±¡ã€‚
        csv_lock (threading.Lock): ç”¨äºä¿æŠ¤CSVå†™å…¥æ“ä½œçš„é”ã€‚
    """
    thread_id = threading.get_ident()
    current_id = str(data_item.get("id", f"no-id-{int(time.time())}"))

    print(f"[{thread_id}] âš™ï¸  å¼€å§‹å¤„ç† ID: {current_id}...")

    node_container = get_nodes(module_type)
    # é€‰æ‹©å·¥ä½œæµ
    test_pipeline = select_flow(module_type,node_container)


    # test_pipeline = create_full_test_pipeline()
    shared = {
        "id": current_id,
        "question": data_item.get("questionï¼ˆçº¯æ–‡æœ¬ï¼‰", data_item.get("question", "N/A")),
        "truth": str(data_item.get('ground_truth', "N/A")),
        "img_url": data_item.get("img_url")
    }
    start_time = time.time()  # ğŸ” è®°å½•å¼€å§‹æ—¶é—´

    try:
        test_pipeline.run(shared)
        result_row = [
            current_id,
            shared.get("question"),
            shared.get("answer", "No_answer"),
            shared.get("distilled_answer", "DISTILL_ERROR"),
            shared.get("truth"), shared.get("final_result", "EVAL_ERROR"),
            shared.get("img_url", "NO_img")
        ]
        print(f"[{thread_id}] âœ… ID: {current_id} å¤„ç†æˆåŠŸã€‚")
    except Exception as e:
        print(f"[{thread_id}] âŒ ID: {current_id} å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        result_row = [
            current_id, shared.get("question"), 'FATAL_ERROR',
            shared.get("truth"), str(e)
        ]
    end_time = time.time()
    duration = end_time - start_time
    # æ·»åŠ å¤„ç†æ—¶é—´åˆ°ç»“æœè¡Œ
    result_row.append(f"{duration:.4f}")  # ä¿ç•™4ä½å°æ•°

    # --- çº¿ç¨‹å®‰å…¨åœ°ã€ç«‹å³å†™å…¥å¹¶åˆ·æ–°åˆ°ç£ç›˜ ---
    # è¿™æ˜¯å®ç°ä¸­æ–­å®‰å…¨çš„å…³é”®
    with csv_lock:
        csv_writer.writerow(result_row)
        f_csv.flush()  # <--- THE FIX: å¼ºåˆ¶å°†ç¼“å†²åŒºå†™å…¥ç£ç›˜

def main():
    start_time = time.time()
    print("ğŸš€ å¼€å§‹è‡ªåŠ¨åŒ–å¹¶å‘æµ‹è¯•æµç¨‹...")

    # --- æ–‡ä»¶è·¯å¾„è®¾ç½® ---
    output_dir = os.path.join(PROJECT_ROOT, 'output_data')
    module_type = os.getenv("MODULE_TYPE", "default_model")

    os.makedirs(output_dir, exist_ok=True)
    base_output_filename = os.path.join(output_dir, f'{FILENAME}_{module_type}_{MODEL_NAME}_å¯¹æ¯”ç»“æœ')
    output_csv_filename = f'{base_output_filename}.csv'

    print(f"ğŸ“‚ æ•°æ®é›†è·¯å¾„: {os.path.join(PROJECT_ROOT, 'data', f'{FILENAME}.json')}")
    print(f"ğŸ“„ CSVè¾“å‡ºè·¯å¾„: {output_csv_filename}")
    print(f"ğŸš€ æ¨¡å‹: {MODEL_NAME}, å¹¶å‘æ•°: {MAX_WORKERS},æ¡†æ¶:{module_type}" )

    # --- æ–­ç‚¹ç»­ä¼ é€»è¾‘ (ä¸å˜) ---
    processed_ids = set()
    if os.path.exists(output_csv_filename):
        print("ğŸ”„ æ£€æµ‹åˆ°è¾“å‡ºæ–‡ä»¶ï¼Œæ¢å¤è¿›åº¦...")
        try:
            with open(output_csv_filename, 'r', encoding='utf-8-sig') as f_read:
                reader = csv.reader(f_read)
                header = next(reader, None)
                if header:
                    for row in reader:
                        if row and row[0]: processed_ids.add(row[0])
            print(f"âœ… æˆåŠŸæ¢å¤ï¼å·²å¤„ç† {len(processed_ids)} ä¸ªæ¡ç›®ã€‚")
        except Exception as e:
            print(f"âš ï¸ è¯»å–CSVæ–‡ä»¶æ¢å¤è¿›åº¦æ—¶å‡ºé”™: {e}")
            processed_ids = set()

    # --- åŠ è½½å¹¶è¿‡æ»¤æ•°æ®é›† (ä¸å˜) ---
    try:
        with open(os.path.join(PROJECT_ROOT, 'data', f'{FILENAME}.json'), 'r', encoding='utf-8') as f:
            all_datasets = json.load(f)
        tasks_to_run = [
            data for i, data in enumerate(all_datasets)
            if str(data.get("id", f"no-id-{i}")) not in processed_ids
        ]
        print(f"ğŸ“š æ•°æ®é›†åŠ è½½æˆåŠŸã€‚å…± {len(all_datasets)} æ¡ï¼Œéœ€å¤„ç† {len(tasks_to_run)} æ¡æ–°ä»»åŠ¡ã€‚")
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        sys.exit(1)

    if not tasks_to_run:
        print("ğŸ‰ æ— æ–°ä»»åŠ¡éœ€è¦å¤„ç†ã€‚ç¨‹åºç»“æŸã€‚")
        sys.exit(0)

    # --- å¹¶å‘æ‰§è¡Œä¸å†™å…¥ (ä¿®å¤) ---
    with open(output_csv_filename, 'a', encoding='utf-8-sig', newline='') as f_csv:
        writer = csv.writer(f_csv)

        f_csv.seek(0, os.SEEK_END)
        if f_csv.tell() == 0:
            writer.writerow(['id', 'problem', 'answer','distilled_answer', 'truth', 'final_result','processing_time'])
            f_csv.flush()   # å†™å…¥è¡¨å¤´åä¹Ÿæœ€å¥½åˆ·æ–°ä¸€ä¸‹

        csv_writer_lock = threading.Lock()

        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # ä¿®æ”¹ç‚¹ï¼šå°† f_csv æ–‡ä»¶å¥æŸ„ä¹Ÿä¼ é€’ç»™å·¥ä½œå‡½æ•°
            futures = [
                executor.submit(process_item_and_write_csv, data, f_csv, writer, csv_writer_lock, module_type)
                for data in tasks_to_run
            ]

            for future in concurrent.futures.as_completed(futures):
                try:
                    future.result()
                except Exception as exc:
                    print(f"ğŸ’¥ [Main Thread] ä¸€ä¸ªå·¥ä½œçº¿ç¨‹å¥”æºƒ: {exc}")

    end_time = time.time()
    print("-" * 60)
    print(f"ğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’ã€‚")
    print(f"ğŸ“„ ç»“æœå·²å…¨éƒ¨ä¿å­˜è‡³: '{output_csv_filename}'")

if __name__ == "__main__":
    main()


