import pocketflow as pf
import openai
import os
import json
import csv
import re
import sys
from dotenv import load_dotenv
from code.ReflectPI.node import ReNode, PINode, ReflectNode, AnswerNode
from utils.llm import call_llm_stream
from utils.pyinterpreter import PythonInterpreter
from utils.prompt_templates import REFLECTPI_RENODE_PROMPT, REFLECTPI_REFLECTNODE_PROMPT, REPI_EVALUATION_NODE_PROMPT, \
    REPI_DISTILL_NODE_PROMPT

# ==============================================================================
# 0. è·¯å¾„å’Œç¯å¢ƒè®¾ç½® (å…³é”®ä¿®æ”¹)
# ==============================================================================
# é€šè¿‡è®¡ç®—è„šæœ¬çš„ç»å¯¹è·¯å¾„æ¥ç¡®ä¿æ— è®ºä»å“ªé‡Œæ‰§è¡Œï¼Œéƒ½èƒ½æ­£ç¡®æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(SCRIPT_DIR, '..', '..'))

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„ï¼Œä»¥ä¾¿èƒ½æˆåŠŸå¯¼å…¥ utils æ¨¡å—
sys.path.append(PROJECT_ROOT)

# ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½ .env æ–‡ä»¶
load_dotenv(os.path.join(PROJECT_ROOT, '.env'))

MODEL_NAME = os.getenv("MODEL_NAME")
MAX_RETRY = int(os.getenv("MAX_RETRY", "3"))


class DistillNode(pf.Node):
    def prep(self, shared):
        """å‡†å¤‡ä»sharedä¸­æå–åŸå§‹ç­”æ¡ˆ"""
        print("ğŸ’§ [DistillNode] é¢„å¤„ç†ï¼Œæå–åŸå§‹ç­”æ¡ˆ...")
        answer = shared.get('answer', 'æœªæ‰¾åˆ°ç­”æ¡ˆ')
        # ä¿å­˜åŸå§‹ç­”æ¡ˆä»¥ä¾›è°ƒè¯•
        shared['answer'] = answer
        return answer

    def exec(self, prep_res):
        """è°ƒç”¨LLMè¿›è¡Œç­”æ¡ˆæçº¯"""
        print("ğŸ’§ [DistillNode] æ‰§è¡Œæçº¯...")
        prompt = REPI_DISTILL_NODE_PROMPT.format(prep_res=prep_res)
        distilled_answer = call_llm_stream(prompt)
        return distilled_answer

    def post(self, shared, prep_res, exec_res):
        print(f"ğŸ’§ [DistillNode] åå¤„ç†ï¼Œæ›´æ–°ç­”æ¡ˆä¸º: '{exec_res}'")
        shared['distilled_answer'] = exec_res


class EvaluationNode(pf.Node):
    def prep(self, shared):
        return {
            "model_answer": shared.get("distilled_answer", "NO_ANSWER_FOUND"),
            "ground_truth": shared.get("truth", "NO_TRUTH_PROVIDED"),
            "question": shared.get("question", "NO_QUESTION_FOUND")
        }

    def exec(self, prep_res):
        # ä½¿ç”¨å¯¼å…¥çš„æç¤ºè¯
        eval_prompt = REPI_EVALUATION_NODE_PROMPT.format(
            model_answer=prep_res["model_answer"],
            ground_truth=prep_res["ground_truth"],
            question=prep_res["question"]
        )
        response = call_llm_stream(eval_prompt)
        if 'ä¸ä¸€è‡´' in response:
            return 'ä¸ä¸€è‡´'
        else:
            return 'ä¸€è‡´'

        return f'EVAL_FORMAT_ERROR: {response}'

    def post(self, shared, prep_res, exec_res):
        shared['final_result'] = exec_res


def create_full_test_pipeline():
    """
    åˆ›å»ºä»è§£é¢˜åˆ°è¯„ä¼°çš„å®Œæ•´è‡ªåŠ¨åŒ–æµ‹è¯•æµç¨‹ã€‚
    """
    # å®ä¾‹åŒ–æ‰€æœ‰èŠ‚ç‚¹
    re = ReNode()
    pi = PINode()
    reflect = ReflectNode()
    answer = AnswerNode()
    distill = DistillNode()
    evaluation = EvaluationNode()

    # å®šä¹‰ ReflectPI Agent å†…éƒ¨çš„å¾ªç¯é€»è¾‘
    re - "calculate" >> pi
    re - "reflect" >> reflect
    re - "answer" >> answer
    pi - "feedback" >> re
    reflect - "feedback" >> re
    reflect - "answer" >> answer

    # å°† Agent çš„å‡ºå£è¿æ¥åˆ°åç»­å¤„ç†èŠ‚ç‚¹ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„æµæ°´çº¿
    answer >> distill >> evaluation

    # è¿”å›ä»¥ ReNode ä¸ºèµ·ç‚¹çš„å®Œæ•´ Flow
    return pf.Flow(start=re)


# ==============================================================================
# 4. æ‰¹å¤„ç†ä¸»ç¨‹åº (å¾ªç¯é€»è¾‘å·²ç®€åŒ–)
# ==============================================================================

if __name__ == '__main__':
    print("ğŸš€ å¼€å§‹è‡ªåŠ¨åŒ–æµ‹è¯•æµç¨‹...")

    test_pipeline = create_full_test_pipeline()
    filename = os.getenv("FILE_NAME", "default_dataset")
    model_name = os.getenv("MODEL_NAME", "default_dataset")

    output_dir = os.path.join(PROJECT_ROOT, 'output_data')
    os.makedirs(output_dir, exist_ok=True)
    base_output_filename = os.path.join(output_dir, f'{filename}_ReflectPI_{MODEL_NAME}_å¯¹æ¯”ç»“æœ')
    output_csv_filename = f'{base_output_filename}.csv'
    output_json_filename = f'{base_output_filename}.json'

    print(f"ğŸ“‚ æ•°æ®é›†è·¯å¾„: {os.path.join(PROJECT_ROOT, 'data', f'{filename}.json')}")
    print(f"ğŸ“„ CSVè¾“å‡ºè·¯å¾„: {output_csv_filename}")
    print(f"ğŸ“„ JSONè¾“å‡ºè·¯å¾„: {output_json_filename}")
    print(f"ğŸš€ æ¨¡å‹ä½¿ç”¨:{model_name}")
    processed_ids = set()
    if os.path.exists(output_csv_filename):
        print(f"æ£€æµ‹åˆ°è¾“å‡ºæ–‡ä»¶ '{os.path.basename(output_csv_filename)}'ï¼Œæ¢å¤è¿›åº¦...")
        try:
            with open(output_csv_filename, 'r', encoding='utf-8-sig') as f:
                reader = csv.reader(f)
                next(reader, None)
                for row in reader:
                    if row: processed_ids.add(row[0])
            print(f"âœ… æˆåŠŸæ¢å¤ï¼å·²å¤„ç† {len(processed_ids)} ä¸ªæ¡ç›®ã€‚")
        except Exception as e:
            print(f"âš ï¸ è¯»å–CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            processed_ids = set()

    try:
        with open(os.path.join(PROJECT_ROOT, 'data', f'{filename}.json'), 'r', encoding='utf-8') as f:
            datasets = json.load(f)
        print(f"ğŸ“š æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œå…± {len(datasets)} æ¡ã€‚")
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        exit()

    json_results = []
    with open(output_csv_filename, 'a', encoding='utf-8-sig', newline='') as f_csv:
        writer = csv.writer(f_csv)

        # --- CSVåˆ—åä¿®æ”¹ç‚¹ ---
        if not processed_ids:
            writer.writerow(['id', 'problem', 'answer', 'truth', 'final'])

        for i, data in enumerate(datasets):
            current_id = str(data.get("id", f"no-id-{i}"))
            if current_id in processed_ids:
                print(f"â­ï¸  ID: {current_id} å·²å¤„ç†ï¼Œè·³è¿‡ã€‚")
                continue

            print("-" * 60)
            print(f"âš™ï¸  æ­£åœ¨å¤„ç†ç¬¬ {i + 1}/{len(datasets)} é¡¹, ID: {current_id}...")

            try:
                shared = {
                    "question": data.get("questionï¼ˆçº¯æ–‡æœ¬ï¼‰", data.get("question", "N/A")),
                    "truth": str(data.get('ground_truth', "N/A"))
                }

                test_pipeline.run(shared)

                # --- CSVè¡Œæ•°æ®ä¿®æ”¹ç‚¹ ---
                result_row = [
                    current_id,
                    shared.get("question", "N/A"),
                    shared.get("answer", "DISTILL_ERROR"),  # 'answer' åˆ—ä½¿ç”¨æçº¯åçš„ç­”æ¡ˆ
                    shared.get("truth", "N/A"),  # 'truth' åˆ—
                    shared.get("final_result", "EVAL_ERROR")  # 'final' åˆ—
                ]
                writer.writerow(result_row)
                f_csv.flush()

                json_results.append(shared.copy())

                print(f"ğŸ ID: {current_id} å®Œæ•´æµç¨‹ç»“æŸã€‚æœ€ç»ˆç»“æœ: {shared.get('final_result', 'EVAL_ERROR')}")

            except Exception as e:
                print(f"âŒ å¤„ç† ID: {current_id} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
                # --- é”™è¯¯è¡Œæ ¼å¼ä¿®æ”¹ç‚¹ ---
                writer.writerow([
                    current_id,
                    data.get("questionï¼ˆçº¯æ–‡æœ¬ï¼‰", "N/A"),
                    'FATAL_ERROR',
                    data.get('ground_truth', 'N/A'),
                    str(e)
                ])
                f_csv.flush()

    if json_results:
        print("-" * 60)
        print(f"ğŸ’¾ æ­£åœ¨å°† {len(json_results)} æ¡è¯¦ç»†ç»“æœå†™å…¥JSONæ–‡ä»¶...")
        with open(output_json_filename, 'w', encoding='utf-8') as f_json:
            json.dump(json_results, f_json, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ JSONæ–‡ä»¶ '{output_json_filename}' ä¿å­˜æˆåŠŸã€‚")

    print("-" * 60)
    print(f"ğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ '{output_csv_filename}' å’Œ '{output_json_filename}'ã€‚")
