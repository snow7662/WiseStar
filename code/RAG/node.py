from pocketflow import Node
from PyPDF2 import PdfReader
from dotenv import load_dotenv
from pdf2image import convert_from_path
import pytesseract
from rag_factory import create_rag_system
from llama_index.core.llms import LLM
from llama_index.core.embeddings import BaseEmbedding

from utils.llm import call_llm_stream
from utils.rag import get_embedding, rerank
from utils.prompt_templates import (
    RAG_RWRITENODE_PROMPT,
    RAG_GENERATE_NODE_PROMPT,
    RAG_SUMMARIZE_PROMPT,
)
from code.RAG.config import (
    CHUNK_SIZE,
    CHUNK_OVERLAP,
    ENCODING,
    EMBEDDING_DIM,
    TOP_K,
    TOP_N,
    CHUNK_SIZE_K,
    EMBEDDING_PROVIDER,
)

import os
import json
import numpy as np
import faiss
from rank_bm25 import BM25Okapi
import jieba
from tqdm import tqdm
import re
import sys

load_dotenv()

"""

æœ¬ç¦»çº¿ç´¢å¼•æµç¨‹çš„èŠ‚ç‚¹ä¹‹é—´çš„è½¬äº¤å€Ÿç”¨æ–‡ä»¶ç³»ç»Ÿå®ç°,æ¯ä¸ªèŠ‚ç‚¹éƒ½åœ¨prepä¸­è¯»æ–‡ä»¶,åœ¨execä¸­æ‰§è¡Œæ‰€éœ€åŠŸèƒ½,åœ¨postä¸­å†™æ–‡ä»¶;
sharedä¸­åªå­˜å‚¨in&outçš„æ–‡ä»¶è·¯å¾„;

RAG schema

shared{
    input_pdf_folder_path:str,å‚è€ƒèµ„æ–™pdfæ‰€åœ¨çš„æ–‡ä»¶å¤¹
    json_path:str,å‚è€ƒèµ„æ–™ç»è¿‡é˜…è¯»å’Œè§£æå¾—åˆ°çš„jsonæ–‡ä»¶
    txt_path:str,å‚è€ƒèµ„æ–™è½¬ä¸ºjsonæ ¼å¼æ‹¼æ¥å¾—åˆ°çš„txtæ–‡ä»¶
    md_path:str,å‚è€ƒèµ„æ–™è½¬ä¸ºmarkdown+latexåå­˜å…¥çš„mdæ–‡ä»¶
    
    chunks_path:str,å¯¹txtè¿›è¡Œå¾ªç¯å­—ç¬¦åˆ†å‰²å¾—åˆ°çš„åˆ‡å—jsonæ–‡ä»¶,.json
    dense_db_path:str,åŸºäºembedding+faisså»ºç«‹çš„ç¨ å¯†å‘é‡åº“è·¯å¾„,.index
    bm25_db_path:str,åŸºäºBM25å»ºç«‹çš„çŸ¥è¯†åº“è·¯å¾„,.json
    cluster_db_path:str,ç±»ä¼¼RAPTORçš„å±‚æ¬¡èšç±»åå¾—åˆ°çš„çŸ¥è¯†åº“è·¯å¾„

    question:str,æ¯æ¡flowåªå¤„ç†ä¸€ä¸ªé—®é¢˜,åœ¨flowå¤–éƒ¨å¹¶å‘
    top_k_docs:List(Dict(id,content)),ç²—æ’æ£€ç´¢ç»“æœ
    top_n_docs:List(Dict(id,content)),ç²¾æ’ç»“æœ
    context:str,æœ€ç»ˆçš„ä¸Šä¸‹æ–‡
    
    solution:str,é¢˜è§£/è§£é¢˜è¿‡ç¨‹
    answer:str,æœ€ç»ˆç­”æ¡ˆ
}
 
"""

# offline nodes=========================================================================================================

"""
ReadNode
- å¤„ç†pdfæ ¼å¼çš„å‚è€ƒèµ„æ–™,å¹¶è¿›è¡Œåˆ†æ®µ
- è¾“å…¥ä¸ºpdfè·¯å¾„
- ä½¿ç”¨pdfé˜…è¯»å·¥å…·,å°†æ–‡ä»¶è¯»æˆstr,å¯¹äºç¼–ç çš„pdf,ä½¿ç”¨PyPDF,å¯¹äºå½±å°å›¾åƒ,ä½¿ç”¨OCRç›¸å…³çš„åº“
- å¯¹è§£æå†…å®¹è¿›è¡Œé‡å†™
- åŠŸèƒ½è¿‡äºè€¦åˆï¼Œåç»­æ‹†åˆ†ä¸ºPdfParserå’ŒRewriteä¸¤éƒ¨åˆ†

"""


class ReadNode(Node):
    def prep(self, shared):
        if "input_pdf_folder_path" not in shared or not shared["input_pdf_folder_path"]:
            raise ValueError(
                "ç¼ºå°‘PDFè¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„ï¼Œè¯·ç¡®ä¿ shared['input_pdf_folder_path'] å·²è®¾ç½®ä¸”éç©º"
            )
        if "md_path" not in shared or not shared["md_path"]:
            raise ValueError("ç¼ºå°‘è¾“å‡ºmdè·¯å¾„ï¼Œè¯·ç¡®ä¿ shared['md_path'] å·²è®¾ç½®ä¸”éç©º")

        in_folder_path = shared["input_pdf_folder_path"]
        pdf_files = [
            f for f in os.listdir(in_folder_path) if f.lower().endswith(".pdf")
        ]
        result = {}
        for fname in tqdm(pdf_files, desc="Processing PDFs"):
            fpath = os.path.join(in_folder_path, fname)
            reader = PdfReader(fpath)
            images = convert_from_path(fpath)
            text_list = []
            for idx, page in enumerate(
                tqdm(reader.pages, desc=f"{fname}", leave=False)
            ):
                page_text = page.extract_text() or ""
                if not page_text.strip():
                    page_img = images[idx]
                    page_text = pytesseract.image_to_string(
                        page_img, lang="chi_sim+eng"
                    )
                text_list.append(page_text)
            title = os.path.splitext(fname)[0]
            result[title] = text_list
        return result

    def exec(self, prep_res):
        def llm_convert(text):
            prompt = RAG_RWRITENODE_PROMPT.format(text=text)
            try:
                latex_text = call_llm_stream(prompt)
            except Exception:
                latex_text = text
            return latex_text

        def extract_result(response):
            match = re.search(r"<result>(.*?)</result>", response, re.DOTALL)
            if match:
                return match.group(1).strip()
            else:
                return response.strip()

        result = {}
        for title, page_text_list in tqdm(
            prep_res.items(), desc="Converting per title"
        ):
            reformatted_list = []
            for page_text in tqdm(page_text_list, desc=f"{title}", leave=False):
                if page_text.strip():
                    response = llm_convert(page_text)
                    reformatted_result = extract_result(response)
                    reformatted_list.append(reformatted_result)
                    reformatted_list.append(page_text)
                else:
                    reformatted_list.append("")
            result[title] = reformatted_list
        return result

    def post(self, shared, prep_res, exec_res):
        if "txt_path" not in shared or not shared["txt_path"]:
            raise ValueError("ç¼ºå°‘è¾“å‡ºtxtè·¯å¾„ï¼Œè¯·ç¡®ä¿ shared['txt_path'] å·²è®¾ç½®ä¸”éç©º")
        if "json_path" not in shared or not shared["json_path"]:
            raise ValueError(
                "ç¼ºå°‘è¾“å‡ºjsonè·¯å¾„ï¼Œè¯·ç¡®ä¿ shared['json_path'] å·²è®¾ç½®ä¸”éç©º"
            )
        if "md_path" not in shared or not shared["md_path"]:
            raise ValueError("ç¼ºå°‘è¾“å‡ºmdè·¯å¾„ï¼Œè¯·ç¡®ä¿ shared['md_path'] å·²è®¾ç½®ä¸”éç©º")
        txt_path = shared["txt_path"]
        json_path = shared["json_path"]
        md_path = shared["md_path"]

        # æ‹¼æ¥å¤§æ–‡æœ¬
        all_text = ""
        all_md = ""
        for title, latex_texts in exec_res.items():
            all_text += f"<SOF>{title}>\n"
            all_md += f"<SOF>{title}>\n"
            for page_text in latex_texts:
                all_text += page_text + "\n"
                # mdæ ¼å¼ï¼šæ¯é¡µç”¨ --- åˆ†éš”ï¼Œmarkdownå¸¸ç”¨ï¼›å¯é…Œæƒ…è°ƒæ•´
                all_md += page_text + "\n\n---\n"
            all_text += f"<EOF>{title}>\n"
            all_md += f"<EOF>{title}>\n"

        # å†™txt
        os.makedirs(os.path.dirname(txt_path), exist_ok=True)
        with open(txt_path, "w", encoding="utf-8") as f:
            f.write(all_text)
        # å†™md
        os.makedirs(os.path.dirname(md_path), exist_ok=True)
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(all_md)
        # å†™json
        os.makedirs(os.path.dirname(json_path), exist_ok=True)
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(prep_res, f, ensure_ascii=False, indent=2)
        return "default"


"""
PdfParseNode
- ä»…è´Ÿè´£PDFè¯»å–å’ŒOCRï¼Œè¾“å‡ºjson
"""


class PdfParseNode(Node):
    """ä»…è´Ÿè´£PDFè¯»å–+OCRï¼Œè¾“å‡ºjsonï¼ˆåˆ†æ­¥å†™ï¼‰"""

    def prep(self, shared):
        input_pdf_folder_path = shared.get("input_pdf_folder_path")
        output_json_path = shared.get("json_path")
        if not output_json_path.lower().endswith(".json"):
            output_json_path += ".json"
            shared["json_path"] = output_json_path
        if not input_pdf_folder_path:
            raise ValueError("ç¼ºå°‘PDFè¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„")
        if not output_json_path:
            raise ValueError("ç¼ºå°‘è¾“å‡ºjsonè·¯å¾„")
        return {
            "input_pdf_folder_path": input_pdf_folder_path,
            "output_json_path": output_json_path,
        }

    def exec(self, prep_res):
        in_folder = prep_res["input_pdf_folder_path"]
        output_json_path = prep_res["output_json_path"]
        pdf_files = [f for f in os.listdir(in_folder) if f.lower().endswith(".pdf")]

        # æ¸…ç©ºæ–‡ä»¶ï¼Œå†™å…¥ {
        os.makedirs(os.path.dirname(output_json_path), exist_ok=True)
        with open(output_json_path, "w", encoding="utf-8") as f:
            f.write("{\n")  # å¼€å¤´

        for idx, fname in enumerate(tqdm(pdf_files, desc="OCR Reading PDFs")):
            fpath = os.path.join(in_folder, fname)
            images = convert_from_path(fpath)
            text_list = []
            for page_img in tqdm(images, desc=f"{fname} OCR", leave=False):
                page_text = pytesseract.image_to_string(page_img, lang="chi_sim+eng")
                text_list.append(page_text)
            title = os.path.splitext(fname)[0]
            # è¿½åŠ å†™å…¥ä¸€é¡¹
            with open(output_json_path, "a", encoding="utf-8") as f:
                key_value = f"  {json.dumps(title, ensure_ascii=False)}: {json.dumps(text_list, ensure_ascii=False)}"
                if idx < len(pdf_files) - 1:
                    f.write(key_value + ",\n")
                else:
                    f.write(key_value + "\n")  # æœ€åä¸€é¡¹ä¸åŠ é€—å·

        # æœ€åå†™å…¥ }
        with open(output_json_path, "a", encoding="utf-8") as f:
            f.write("}\n")

    def post(self, shared, prep_res, exec_res):
        return "default"


"""
RewriteNode
- å¯¹è§£æå†…å®¹è¿›è¡Œé‡å†™ï¼Œä¿å­˜è‡³txtå’Œmdæ–‡ä»¶
"""


class RewriteNode(Node):
    def prep(self, shared):
        input_json_path = shared.get("json_path")
        output_md_path = shared.get("md_path")
        if not output_md_path.lower().endswith(".md"):
            output_md_path += ".md"
            shared["md_path"] = output_md_path
        output_txt_path = shared.get("txt_path")
        if not output_txt_path.lower().endswith(".txt"):
            output_txt_path += ".txt"
            shared["txt_path"] = output_txt_path
        if not input_json_path:
            raise ValueError("ç¼ºå°‘è¾“å…¥jsonè·¯å¾„")
        if not output_md_path:
            raise ValueError("ç¼ºå°‘è¾“å‡ºmdè·¯å¾„")
        if not output_txt_path:
            raise ValueError("ç¼ºå°‘è¾“å‡ºtxtè·¯å¾„")
        return {
            "input_json_path": input_json_path,
            "output_md_path": output_md_path,
            "output_txt_path": output_txt_path,
        }

    def exec(self, prep_res):
        input_json_path = prep_res["input_json_path"]
        output_md_path = prep_res["output_md_path"]
        output_txt_path = prep_res["output_txt_path"]
        os.makedirs(os.path.dirname(output_md_path), exist_ok=True)
        os.makedirs(os.path.dirname(output_txt_path), exist_ok=True)

        with open(input_json_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        def llm_convert(text):
            prompt = RAG_RWRITENODE_PROMPT.format(text=text)
            try:
                # ä½¿ç”¨turboæ¨¡å‹è¿›è¡Œé‡å†™
                latex_text = call_llm_stream(
                    prompt, model_name=os.getenv("REWRITE_MODEL_NAME")
                )
                match = re.search(r"<result>(.*?)</result>", latex_text, re.DOTALL)
                if match:
                    latex_text = match.group(1)

            except Exception:
                latex_text = text
            return latex_text

        with (
            open(output_md_path, "a", encoding="utf-8") as fmd,
            open(output_txt_path, "a", encoding="utf-8") as ftxt,
        ):
            for title, page_list in tqdm(
                data.items(),
                desc="LLM Rewrite per file",
                file=sys.stdout,
                dynamic_ncols=True,
            ):
                new_chunk_list = []
                for i in tqdm(
                    range(0, len(page_list), CHUNK_SIZE_K),
                    desc=f"Processing '{title}' in chunks",
                    leave=False,
                    file=sys.stdout,
                    dynamic_ncols=True,
                ):
                    chunk_of_pages = page_list[i : i + CHUNK_SIZE_K]
                    separator = "\n\n---\n\n"
                    combined_text = separator.join(
                        [p.strip() for p in chunk_of_pages if p.strip()]
                    )
                    if combined_text:
                        rewritten_chunk = llm_convert(combined_text)
                        new_chunk_list.append(rewritten_chunk)

                # å†™å…¥ç£ç›˜
                fmd.write(f"# {title}\n\n")
                ftxt.write(f"<SOF>{title}>\n")
                for idx, page in enumerate(new_chunk_list):
                    fmd.write(f"## Page {idx + 1}\n\n{page}\n\n")
                    ftxt.write(page + "\n")
                ftxt.write(f"<EOF>{title}>\n")

    def post(self, shared, prep_res, exec_res):
        return "default"


"""
RecursiveChunkNode
    - ç±»ä¼¼langchainçš„å¾ªç¯å­—ç¬¦åˆ‡å—æ–¹å¼,æŒ‰ç…§chunk_sizeå’Œoverlap_sizeè¿›è¡Œåˆ‡åˆ†
    - åˆ‡å®Œçš„chunkså½¢å¼ä¸Šä¸ºList(id:int,content:str),å…¶ä¸­idä¸ºä»0å¼€å§‹çš„é¡ºåºæ ‡è®°
    - å°†chunkså­˜å‚¨ä¸ºjsonæ ¼å¼çš„æ–‡ä»¶,å†™å…¥chunks_pathä¸­
"""


class RecursiveChunkNode(Node):
    def __init__(self):
        super().__init__()
        self.chunk_size = CHUNK_SIZE
        self.overlap = CHUNK_OVERLAP
        self.encoding = ENCODING
        self.separators = [
            "\n\n",
            "\n",
            "ã€‚",
            "ï¼",
            "ï¼Ÿ",
            ".",
            "!",
            "?",
            "ï¼Œ",
            ",",
            " ",
            "",
        ]  # ä¸­è‹±æ–‡æ ‡ç‚¹æ”¯æŒ

    def prep(self, shared):
        txt_path = shared["txt_path"]
        with open(txt_path, "r", encoding=self.encoding) as f:
            reformat_txt = f.read()
        return reformat_txt

    def exec(self, prep_res):
        text = prep_res

        def recursive_split(text, separators, chunk_size):
            if not separators:
                # æœ€ç»†ä¸èƒ½å†åˆ†ç›´æ¥ç¡¬åˆ‡
                return [
                    text[i : i + chunk_size] for i in range(0, len(text), chunk_size)
                ]
            sep = separators[0]
            # å¦‚æœæ‰¾ä¸åˆ°åˆ†éš”ç¬¦ï¼Œç»§ç»­é€’å½’ä¸‹ä¸€ä¸ª
            if sep and (sep in text):
                parts = text.split(sep)
                chunks = []
                tmp = ""
                for idx, part in enumerate(parts):
                    if tmp:
                        # é¢„ä¼°åŠ ä¸Šåˆ†éš”ç¬¦é•¿åº¦
                        next_len = len(tmp) + len(sep) + len(part)
                    else:
                        next_len = len(part)
                    if tmp and next_len > chunk_size:
                        # å½“å‰tmpå·²æ»¡,æ¨å…¥
                        chunks.append(tmp)
                        tmp = part
                    else:
                        if tmp:
                            tmp += sep + part
                        else:
                            tmp = part
                if tmp:
                    chunks.append(tmp)
                # é’ˆå¯¹æ¯ä¸ªè¶…è¿‡sizeçš„é€’å½’ç»†åˆ†
                result = []
                for c in chunks:
                    if len(c) > chunk_size and len(separators) > 1:
                        result += recursive_split(c, separators[1:], chunk_size)
                    else:
                        result.append(c)
                return result
            else:
                # sepæ— æ•ˆï¼Œé€’å½’ä¸‹ä¸€çº§
                return recursive_split(text, separators[1:], chunk_size)

        all_chunks = recursive_split(text, self.separators, self.chunk_size)
        chunks_result = []

        for i, chunk in enumerate(all_chunks):
            if not chunk.strip():
                continue
            # overlapå¤„ç†
            prev_content = ""
            overlap_len = 0
            j = i - 1
            while j >= 0 and overlap_len < self.overlap:
                prev_chunk = all_chunks[j]
                need = min(self.overlap - overlap_len, len(prev_chunk))
                prev_content = prev_chunk[-need:] + prev_content
                overlap_len += need
                j -= 1
            # æ‹¼æˆå¸¦overlapçš„æ–‡æœ¬
            merged = prev_content + chunk
            # ä¿è¯ä¸è¶…å‡ºchunk_size
            merged = merged[-self.chunk_size :]
            chunks_result.append(
                {
                    "id": i,
                    "content": merged,
                }
            )

        return chunks_result

    def post(self, shared, prep_res, exec_res):
        chunks_path = shared["chunks_path"]
        # è‡ªåŠ¨æ·»åŠ  .json åç¼€
        if not chunks_path.lower().endswith(".json"):
            chunks_path += ".json"
            # å¯é€‰ï¼šåŒæ­¥æ›´æ–° sharedï¼ˆä¸‹æ¸¸ç”¨åˆ°æ—¶æ›´ä¸€è‡´ï¼‰
            shared["chunks_path"] = chunks_path
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(chunks_path), exist_ok=True)
        with open(chunks_path, "w", encoding=self.encoding) as f:
            json.dump(exec_res, f, ensure_ascii=False, indent=2)
        return "default"


"""
EmbedNode
    - ä½¿ç”¨embedding modelç”Ÿæˆç¨ å¯†å‘é‡,å¹¶å†™å…¥faisså‘é‡æ•°æ®åº“
"""


class EmbedNode(Node):
    def prep(self, shared):
        # ä»shared["chunks_path"]ä¸­è¯»å–chunks
        with open(shared["chunks_path"], "r", encoding=ENCODING) as f:
            chunks = json.load(f)
        return chunks

    def exec(self, prep_res):
        texts = [chunk["content"] for chunk in prep_res]
        ids = [chunk["id"] for chunk in prep_res]
        # åˆ†æ‰¹æ¯æ¬¡ä¸è¶…è¿‡ 10 æ¡
        BATCH = 10
        all_embeddings = []
        for i in tqdm(range(0, len(texts), BATCH), desc="Embedding Batches"):
            batch_texts = texts[i : i + BATCH]
            batch_embeddings = get_embedding(batch_texts, dimensions=EMBEDDING_DIM)
            all_embeddings.extend(batch_embeddings)
        assert len(all_embeddings) == len(texts), "Mismatch in embedding count"
        return {
            "ids": ids,
            "embeddings": all_embeddings,
        }

    def post(self, shared, prep_res, exec_res):
        # å°†embeddingå†™å…¥FAISSå‘é‡æ•°æ®åº“
        # FAISSå†™å…¥
        ids = np.array(exec_res["ids"]).astype("int64")
        embs = np.array(exec_res["embeddings"]).astype("float32")
        db_path = shared["dense_db_path"]
        dim = embs.shape[1]

        # è‡ªåŠ¨æ·»åŠ  .index åç¼€
        if not db_path.lower().endswith(".index"):
            db_path += ".index"
            # å¯é€‰ï¼šæ›´æ–° shared["dense_db_path"]ï¼Œä¾¿äºåç»­èŠ‚ç‚¹ç»Ÿä¸€å¼•ç”¨
            shared["dense_db_path"] = db_path

        # å»ºç«‹FAISSç´¢å¼•
        index = faiss.IndexIDMap(faiss.IndexFlatL2(dim))
        index.add_with_ids(embs, ids)

        faiss.write_index(index, db_path)
        print(f"FAISS index saved to {db_path}")

        return "default"


"""
BM25Node
    - ä½¿ç”¨BM25ç”Ÿæˆç¨€ç–å‘é‡
"""


class BM25Node(Node):
    def prep(self, shared):
        with open(shared["chunks_path"], "r", encoding="utf-8") as f:
            chunks = json.load(f)
        return chunks

    def exec(self, prep_res):
        # 1. åˆ†è¯å¹¶æ„å»º tokens list
        docs = prep_res
        # tokens æ”¯æŒè‡ªå®šä¹‰åˆ†è¯ï¼Œè¿™é‡Œç”¨ jiebaï¼Œé€‚åˆä¸­æ–‡
        for doc in docs:
            doc["tokens"] = list(jieba.cut(doc["content"]))
        # 2. æŒ‰éœ€æ±‚è¿”å› tokens æ•°æ®
        return docs

    def post(self, shared, prep_res, exec_res):
        # ä¿å­˜ä¸ºvectoråº“ (æœ¬åœ°jsonå­˜["id","content","tokens"])
        bm25_db_path = shared["bm25_db_path"]  # å¦‚ bm25_vec.json
        if not bm25_db_path.lower().endswith(".json"):
            bm25_db_path += ".json"
            # æ›´æ–° sharedï¼Œè‹¥åç»­èŠ‚ç‚¹è¿˜ä¼šç”¨åˆ° bm25_db_path
            shared["bm25_db_path"] = bm25_db_path
        with open(bm25_db_path, "w", encoding="utf-8") as f:
            json.dump(exec_res, f, ensure_ascii=False, indent=2)

        print(f"BM25 vector saved to {bm25_db_path}")
        return "default"



class RAGNode(Node):
    def __init__(
        self,
        llm: LLM,
        embed_model: BaseEmbedding,
        rag_type: str = "raptor",
        docs: list[str] | None = [],
        reindex: bool = False,
        **kwargs
    ):
        super().__init__()
        self.rag = create_rag_system(llm, embed_model, rag_type, docs, reindex=reindex, **kwargs)

    def prep(self, shared):
        question = shared["question"]
        topk = shared.get("rag_topk", 10)
        return {"question": question, "topk": topk}

    def exec(self, prep_res):
        print("executing RAG...")
        query = prep_res["question"]
        topk = prep_res["topk"]
        nodes_with_score = self.rag.query(query, topk)
        result = [
            {"text": node["text"], "score": node["score"]} for node in nodes_with_score
        ]
        print("result:", result)
        return {"result": result}

    def post(self, shared, prep_res, exec_res):
        result = exec_res["result"]
        print(f"Fetched nodes: {len(result)}")
        shared["rag_result"] = result
        shared["context"] = "\n".join([info["text"] for info in result])
        return "default"


# online nodes=========================================================================================================


class DenseRetrieveNode(Node):
    def prep(self, shared):
        # è¯»å–å½“å‰question
        question = shared["question"]
        db_path = shared["dense_db_path"]
        if not os.path.exists(db_path):
            print(f"æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
            return None
        # è®¡ç®—åµŒå…¥
        question_embedding = get_embedding([question], dimensions=EMBEDDING_DIM)[
            0
        ]  # å–ç¬¬ä¸€ä¸ªå…ƒç´ 
        return {
            "question": question,
            "question_embedding": question_embedding,
            "db_path": db_path,
        }

    def exec(self, prep_res):
        # åœ¨FAISSåº“ä¸­æŸ¥top k

        index = faiss.read_index(prep_res["db_path"])

        emb = np.array([prep_res["question_embedding"]]).astype("float32")
        similarity, related_ids = index.search(emb, TOP_K)
        ids = related_ids[0].tolist()
        return {"related_docs_id": ids}

    def post(self, shared, prep_res, exec_res):
        # æŸ¥æ‰¾chunkså†…å®¹ï¼Œè¾“å‡ºtop k docs
        with open(shared["chunks_path"], "r", encoding="utf-8") as f:
            all_chunks = json.load(f)
        id2chunk = {chunk["id"]: chunk["content"] for chunk in all_chunks}
        # æŒ‰é¡ºåºç»„è£… [{id, content}]
        docs = [
            {"id": _id, "content": id2chunk.get(_id, "")}
            for _id in exec_res["related_docs_id"]
        ]
        shared["top_k_docs"] = docs
        return "default"


class BM25RetrieveNode(Node):
    def prep(self, shared):
        # è¯»å–åˆ†è¯åçš„bm25åº“ï¼ˆbm25_vec.jsonï¼‰ï¼Œå†…å®¹ä¸º[{id, content, tokens}]
        question = shared["question"]
        db_path = shared["bm25_db_path"]
        with open(db_path, "r", encoding="utf-8") as f:
            docs = json.load(f)
        return {"docs": docs, "question": question}

    def exec(self, prep_res):
        """
        å¯¹ shared["question"] çš„é—®é¢˜è¿›è¡Œ BM25 æ£€ç´¢
        """
        docs = prep_res["docs"]
        question = prep_res["question"]
        corpus = [doc["tokens"] for doc in docs]
        bm25 = BM25Okapi(corpus)
        q_tokens = list(jieba.cut(question))
        scores = bm25.get_scores(q_tokens)
        ranked = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[
            :TOP_K
        ]
        docs_result = [
            {
                "id": docs[idx]["id"],
                "content": docs[idx]["content"],
                # "score": float(scores[idx]),
            }
            # docs[idx]["content"]
            for idx in ranked
        ]
        return docs_result

    def post(self, shared, prep_res, exec_res):
        # æ£€ç´¢ç»“æœå†™å…¥ shared
        shared["top_k_docs"] = exec_res  # å•æ¡list
        return "default"


"""
HybridRetrieveNode
æ··åˆdenseå’Œbm25æ£€ç´¢,æ£€ç´¢åå»é‡
"""


class HybridRetrieveNode(Node):
    def prep(self, shared):
        question = shared["question"]
        dense_db_path = shared["dense_db_path"]
        bm25_db_path = shared["bm25_db_path"]
        chunks_path = shared["chunks_path"]
        return {
            "question": question,
            "dense_db_path": dense_db_path,
            "bm25_db_path": bm25_db_path,
            "chunks_path": chunks_path,
        }

    def exec(self, prep_res):
        question = prep_res["question"]
        # ------- denseæ£€ç´¢ -------
        dense_embedding = get_embedding([question], dimensions=EMBEDDING_DIM)[0]
        dense_index = faiss.read_index(prep_res["dense_db_path"])
        emb = np.array([dense_embedding]).astype("float32")
        _, dense_ids = dense_index.search(emb, TOP_K)
        dense_ids = dense_ids[0].tolist()

        # ------- bm25æ£€ç´¢ -------
        with open(prep_res["bm25_db_path"], "r", encoding="utf-8") as f:
            bm25_docs = json.load(f)
        corpus = [doc["tokens"] for doc in bm25_docs]
        bm25 = BM25Okapi(corpus)
        q_tokens = list(jieba.cut(question))
        scores = bm25.get_scores(q_tokens)
        bm25_ranked = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[
            :TOP_K
        ]
        bm25_ids = [bm25_docs[idx]["id"] for idx in bm25_ranked]

        # ------- åˆå¹¶å»é‡ -------
        all_ids = []
        seen = set()
        # ä¿è¯denseä¼˜å…ˆï¼Œé¡ºåºä¾æ¬¡å–denseå’Œbm25ï¼Œå·²å‡ºç°çš„ä¸å†æ”¾å…¥
        for _id in dense_ids + bm25_ids:
            if _id not in seen:
                all_ids.append(_id)
                seen.add(_id)

        return {"related_docs_id": all_ids}

    def post(self, shared, prep_res, exec_res):
        # æŸ¥å›å†…å®¹ï¼ŒList[Dict(id, content)]
        with open(prep_res["chunks_path"], "r", encoding="utf-8") as f:
            all_chunks = json.load(f)
        id2chunk = {chunk["id"]: chunk["content"] for chunk in all_chunks}
        docs = [
            {"id": _id, "content": id2chunk.get(_id, "")}
            for _id in exec_res["related_docs_id"]
        ]
        shared["top_k_docs"] = docs
        return "default"


class RerankNode(Node):
    def prep(self, shared):
        # print("ğŸ’¬[RerankNode]å¤„ç†ä¸­...")

        question = shared.get("question")
        top_k_docs = shared.get("top_k_docs")
        return {"question": question, "top_k_docs": top_k_docs}

    def exec(self, prep_res):
        question = prep_res["question"]
        top_k_docs = prep_res["top_k_docs"]
        content2doc = {doc["content"]: doc for doc in top_k_docs}
        contents = [doc["content"] for doc in top_k_docs]
        reranked_contents = rerank(question, contents, TOP_N)
        top_n_docs = [content2doc[c] for c in reranked_contents if c in content2doc]
        return top_n_docs

    def post(self, shared, prep_res, exec_res):
        shared["top_n_docs"] = exec_res

        # print("ğŸ’¬[RerankNode]å¤„ç†ç»“æŸ.")
        return "default"


class SummarizeNode(Node):
    """
    å¯¹ä¸Šä¸‹æ–‡è¿›è¡Œé‡å†™
    """

    def prep(self, shared):
        # print("ğŸ’¬[SummarizeNode]å¤„ç†ä¸­...")

        question = shared.get("question")
        docs = []
        # ä¼˜å…ˆé€‰å–ç²¾æ’ç»“æœ,å¦åˆ™é€‰æ‹©ç²—æ’ç»“æœ
        if shared.get("top_n_docs"):
            docs = shared["top_n_docs"]
        elif shared.get("top_k_docs"):
            docs = shared.get("top_k_docs")
        else:
            raise Exception("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£")

        return {"question": question, "docs": docs}

    def exec(self, prep_res):
        question = prep_res["question"]
        docs = prep_res["docs"]
        context = ""
        # ä½¿ç”¨LLMè¿›è¡Œæ€»ç»“å»é‡
        prompt = RAG_SUMMARIZE_PROMPT.format(
            question=question, docs="\n".join([doc["content"] for doc in docs])
        )
        response = call_llm_stream(prompt)
        match = re.search(r"<context>(.*?)</context>", response, re.DOTALL)
        if match:
            context = match.group(1)
        return context

    def post(self, shared, prep_res, exec_res):
        shared["context"] = exec_res

        # print(f"ğŸ’¬[SummarizeNode]å¤„ç†ç»“æŸ.")
        return "default"


class GenerateNode(Node):
    def prep(self, shared):
        """
        ç”Ÿæˆç­”æ¡ˆ
        """
        # print(f"ğŸ’¬[GenerateNode]ç”Ÿæˆç­”æ¡ˆä¸­...")
        # ä¼˜å…ˆé€‰å–ç²¾æ’ç»“æœ,å¦åˆ™é€‰å–ç²—æ’ç»“æœ
        # related_docs = shared.get("top_n_docs") if shared.get("top_n_docs") else shared.get("top_k_docs")
        question = shared.get("question")

        context = ""
        if shared.get("context"):
            context = shared.get("context")
        elif shared.get("top_n_docs"):
            context = "\n".join([doc["content"] for doc in shared.get("top_n_docs")])
        elif shared.get("top_k_docs"):
            context = "\n".join([doc["content"] for doc in shared.get("top_k_docs")])

        # å†™å…¥context
        shared["context"] = context

        return {"context": context, "question": question}

    def exec(self, prep_res):
        # æ‹¼æ¥ä¸Šä¸‹æ–‡
        # context = "\n".join(prep_res["related_docs"])
        context = prep_res["context"]
        prompt = RAG_GENERATE_NODE_PROMPT.format(
            question=prep_res["question"], context=context
        )
        solution = ""
        answer = ""

        response = call_llm_stream(prompt)
        solution_match = re.search(r"<solution>(.*?)</solution>", response, re.DOTALL)
        answer_match = re.search(r"<answer>(.*?)</answer", response, re.DOTALL)
        if solution_match:
            solution = solution_match.group(1)
        if answer_match:
            answer = answer_match.group(1)

        return {"solution": solution, "answer": answer}

    def post(self, shared, prep_res, exec_res):
        shared["solution"] = exec_res["solution"]
        shared["answer"] = exec_res["answer"]

        # print(f"ğŸ’¬[GenerateNode]ç”Ÿæˆç­”æ¡ˆç»“æŸ.")
        return "default"
